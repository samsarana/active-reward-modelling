\BOOKMARK [-1][-]{part.1}{I Background}{}% 1
\BOOKMARK [0][-]{chapter.1}{Introduction}{part.1}% 2
\BOOKMARK [1][-]{section.1.1}{Relation to Material Studied on the MSc Course}{chapter.1}% 3
\BOOKMARK [0][-]{chapter.2}{Neural Networks}{part.1}% 4
\BOOKMARK [0][-]{chapter.3}{Reinforcement Learning}{part.1}% 5
\BOOKMARK [1][-]{section.3.1}{Elements of Reinforcement Learning}{chapter.3}% 6
\BOOKMARK [1][-]{section.3.2}{Finite Markov Decision Processes}{chapter.3}% 7
\BOOKMARK [2][-]{subsection.3.2.1}{The Agent-Environment Interface}{section.3.2}% 8
\BOOKMARK [2][-]{subsection.3.2.2}{Goals and Rewards}{section.3.2}% 9
\BOOKMARK [2][-]{subsection.3.2.3}{Returns and Episodes}{section.3.2}% 10
\BOOKMARK [2][-]{subsection.3.2.4}{Policies and Value Functions}{section.3.2}% 11
\BOOKMARK [2][-]{subsection.3.2.5}{Optimal Policies and Optimal Value Functions}{section.3.2}% 12
\BOOKMARK [2][-]{subsection.3.2.6}{Bellman Equations}{section.3.2}% 13
\BOOKMARK [1][-]{section.3.3}{Reinforcement Learning Solution Methods}{chapter.3}% 14
\BOOKMARK [2][-]{subsection.3.3.1}{Taxonomy of RL Solution Methods}{section.3.3}% 15
\BOOKMARK [2][-]{subsection.3.3.2}{Deep Q-network}{section.3.3}% 16
\BOOKMARK [1][-]{section.3.4}{Reinforcement Learning from Unknown Reward Functions}{chapter.3}% 17
\BOOKMARK [2][-]{subsection.3.4.1}{Reward Learning from Trajectory Preferences with Handcrafted Feature Transformations}{section.3.4}% 18
\BOOKMARK [2][-]{subsection.3.4.2}{Reward Learning from Trajectory Preferences in Deep RL}{section.3.4}% 19
\BOOKMARK [0][-]{chapter.4}{Uncertainty in Deep Learning}{part.1}% 20
\BOOKMARK [1][-]{section.4.1}{Bayesian Neural Networks}{chapter.4}% 21
\BOOKMARK [1][-]{section.4.2}{Model Uncertainty in BNNs}{chapter.4}% 22
\BOOKMARK [0][-]{chapter.5}{Active Learning}{part.1}% 23
\BOOKMARK [1][-]{section.5.1}{Acquisition Functions}{chapter.5}% 24
\BOOKMARK [2][-]{subsection.5.1.1}{Max Entropy}{section.5.1}% 25
\BOOKMARK [2][-]{subsection.5.1.2}{Variation Ratios}{section.5.1}% 26
\BOOKMARK [2][-]{subsection.5.1.3}{Mean STD}{section.5.1}% 27
\BOOKMARK [2][-]{subsection.5.1.4}{BALD}{section.5.1}% 28
\BOOKMARK [1][-]{section.5.2}{Applying Active Learning to RL without a reward function}{chapter.5}% 29
\BOOKMARK [2][-]{subsection.5.2.1}{APRIL}{section.5.2}% 30
\BOOKMARK [2][-]{subsection.5.2.2}{Active Preference-Based Learning of Reward Functions with handcrafted feature transformations}{section.5.2}% 31
\BOOKMARK [2][-]{subsection.5.2.3}{Deep RL from Human Preferences}{section.5.2}% 32
\BOOKMARK [-1][-]{part.2}{II Innovation}{}% 33
\BOOKMARK [0][-]{chapter.6}{Method}{part.2}% 34
\BOOKMARK [1][-]{section.6.1}{Possible failure modes of active reward modelling}{chapter.6}% 35
\BOOKMARK [1][-]{section.6.2}{Applying acquisition functions to reward modelling}{chapter.6}% 36
\BOOKMARK [1][-]{section.6.3}{ARMA}{chapter.6}% 37
\BOOKMARK [1][-]{section.6.4}{Acquisition Functions}{chapter.6}% 38
\BOOKMARK [1][-]{section.6.5}{Uncertainty Estimates}{chapter.6}% 39
\BOOKMARK [1][-]{section.6.6}{Implementation Details}{chapter.6}% 40
\BOOKMARK [0][-]{chapter.7}{Experimental Details}{part.2}% 41
\BOOKMARK [1][-]{section.7.1}{CartPole-v0}{chapter.7}% 42
\BOOKMARK [0][-]{chapter.8}{Results}{part.2}% 43
\BOOKMARK [1][-]{section.8.1}{CartPole-v0}{chapter.8}% 44
\BOOKMARK [0][-]{chapter.9}{Conclusions}{part.2}% 45
\BOOKMARK [1][-]{section.9.1}{Summary}{chapter.9}% 46
\BOOKMARK [1][-]{section.9.2}{Evaluation}{chapter.9}% 47
\BOOKMARK [1][-]{section.9.3}{Future Work}{chapter.9}% 48
\BOOKMARK [0][-]{chapter*.13}{References}{part.2}% 49
\BOOKMARK [0][-]{appendix.A}{Some Appendix Material}{part.2}% 50
