\BOOKMARK [-1][-]{part.1}{I Background}{}% 1
\BOOKMARK [0][-]{chapter.1}{Introduction}{part.1}% 2
\BOOKMARK [1][-]{section.1.1}{Relation to Material Studied on the MSc Course}{chapter.1}% 3
\BOOKMARK [0][-]{chapter.2}{Reinforcement Learning}{part.1}% 4
\BOOKMARK [1][-]{section.2.1}{Elements of Reinforcement Learning}{chapter.2}% 5
\BOOKMARK [1][-]{section.2.2}{Finite Markov Decision Processes}{chapter.2}% 6
\BOOKMARK [2][-]{subsection.2.2.1}{The Agent-Environment Interface}{section.2.2}% 7
\BOOKMARK [2][-]{subsection.2.2.2}{Goals and Rewards}{section.2.2}% 8
\BOOKMARK [2][-]{subsection.2.2.3}{Returns and Episodes}{section.2.2}% 9
\BOOKMARK [2][-]{subsection.2.2.4}{Policies and Value Functions}{section.2.2}% 10
\BOOKMARK [2][-]{subsection.2.2.5}{Optimal Policies and Optimal Value Functions}{section.2.2}% 11
\BOOKMARK [2][-]{subsection.2.2.6}{Bellman Equations}{section.2.2}% 12
\BOOKMARK [1][-]{section.2.3}{Reinforcement Learning Solution Methods}{chapter.2}% 13
\BOOKMARK [2][-]{subsection.2.3.1}{Taxonomy of RL Solution Methods}{section.2.3}% 14
\BOOKMARK [2][-]{subsection.2.3.2}{Deep Q-network}{section.2.3}% 15
\BOOKMARK [1][-]{section.2.4}{Reinforcement Learning from Unknown Reward Functions}{chapter.2}% 16
\BOOKMARK [2][-]{subsection.2.4.1}{Reward Learning from Trajectory Preferences with Handcrafted Feature Transformations}{section.2.4}% 17
\BOOKMARK [2][-]{subsection.2.4.2}{Reward Learning from Trajectory Preferences in Deep RL}{section.2.4}% 18
\BOOKMARK [0][-]{chapter.3}{Uncertainty in Deep Learning}{part.1}% 19
\BOOKMARK [1][-]{section.3.1}{Bayesian Neural Networks}{chapter.3}% 20
\BOOKMARK [1][-]{section.3.2}{Model Uncertainty in BNNs}{chapter.3}% 21
\BOOKMARK [0][-]{chapter.4}{Active Learning}{part.1}% 22
\BOOKMARK [1][-]{section.4.1}{Acquisition Functions}{chapter.4}% 23
\BOOKMARK [2][-]{subsection.4.1.1}{Max Entropy}{section.4.1}% 24
\BOOKMARK [2][-]{subsection.4.1.2}{Variation Ratios}{section.4.1}% 25
\BOOKMARK [2][-]{subsection.4.1.3}{Mean STD}{section.4.1}% 26
\BOOKMARK [2][-]{subsection.4.1.4}{BALD}{section.4.1}% 27
\BOOKMARK [1][-]{section.4.2}{Applying Active Learning to RL without a reward function}{chapter.4}% 28
\BOOKMARK [2][-]{subsection.4.2.1}{APRIL}{section.4.2}% 29
\BOOKMARK [2][-]{subsection.4.2.2}{Active Preference-Based Learning of Reward Functions with handcrafted feature transformations}{section.4.2}% 30
\BOOKMARK [2][-]{subsection.4.2.3}{Deep RL from Human Preferences}{section.4.2}% 31
\BOOKMARK [0][-]{chapter.5}{Method}{part.1}% 32
\BOOKMARK [1][-]{section.5.1}{Training Protocol}{chapter.5}% 33
\BOOKMARK [1][-]{section.5.2}{Acquisition Functions}{chapter.5}% 34
\BOOKMARK [1][-]{section.5.3}{Uncertainty Estimates}{chapter.5}% 35
\BOOKMARK [1][-]{section.5.4}{Implementation Details}{chapter.5}% 36
\BOOKMARK [0][-]{chapter.6}{Experimental Details}{part.1}% 37
\BOOKMARK [1][-]{section.6.1}{CartPole-v0}{chapter.6}% 38
\BOOKMARK [0][-]{chapter.7}{Results}{part.1}% 39
\BOOKMARK [1][-]{section.7.1}{CartPole-v0}{chapter.7}% 40
\BOOKMARK [0][-]{chapter.8}{Conclusions}{part.1}% 41
\BOOKMARK [1][-]{section.8.1}{Summary}{chapter.8}% 42
\BOOKMARK [1][-]{section.8.2}{Evaluation}{chapter.8}% 43
\BOOKMARK [1][-]{section.8.3}{Future Work}{chapter.8}% 44
\BOOKMARK [0][-]{chapter*.13}{References}{part.1}% 45
\BOOKMARK [0][-]{appendix.A}{Some Appendix Material}{part.1}% 46
