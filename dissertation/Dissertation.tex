\documentclass[11pt, a4paper, bibliography=totoc]{report}
\usepackage{amsmath,amsthm,amssymb, tabularx, listings, enumerate, cancel, bussproofs, tabto, wasysym, algpseudocode, algorithm, savesym, mathtools, physics, xcolor, setspace, appendix}
\usepackage[margin=3.5cm]{geometry}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage[nottoc]{tocbibind} % adds Bibliography to 
\usepackage[linktocpage]{hyperref}

\hypersetup{
	colorlinks,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}
\doublespacing
\pagestyle{headings}

\newcommand{\nats}{\mathbb{N}}
\newcommand{\reals}{\mathbb{R}}
\renewcommand{\P}[1]{\mathbb{P}\left( #1 \right) }
%\newcommand{\E}[1]{\mathbb{E} \left[ #1 \right] }
\newcommand{\E}[2]{\mathbb{E}_{#1} \left[ #2 \right] }
\newcommand{\V}[1]{\mathbb{V}ar \left[ #1 \right]}
\newcommand{\KLD}[2]{\mathrm{KL} \left[ \left. \left. #1 \right|\right| #2 \right] }

\newcommand{\w}{\mathbf{w}}
\newcommand{\data}{\mathcal{D}}
\newcommand{\vfe}{\mathcal{F}(\data, \theta)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\bepsilon}{\pmb{\epsilon}}
\newcommand{\bmu}{\pmb{\mu}}
\newcommand{\bsigma}{\pmb{\sigma}}
\newcommand{\btheta}{\pmb{\theta}}
\newcommand{\brho}{\pmb{\rho}}
\newcommand{\normal}[3]{\mathcal{N}(#1 \vert #2, #3)}
\newtheorem*{claim}{Claim}
\newtheorem*{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem*{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

\begin{document}
\title{Active Learning for Reward Modelling}
\author{Sam Clarke}
\date{September 2019}
\renewcommand{\bibname}{References}
\maketitle

\begin{abstract} % ~200 words

\end{abstract}

\tableofcontents
% say somewhere around here about collaboration with Zac and Angelos
% and where do I say who my supervisor is?

\chapter{Introduction}

\section{Relation to Material Studied on the MSc Course}

\chapter{Reinforcement Learning} % Explain problem, give context
Reinforcement learning (RL) refers simultaneously to a problem, methods for solving that problem, and the field that studies the problem and its solution methods. The problem of RL is to learn what to do---how to map situations to actions---so as to maximise some numerical reward signal \cite[pp.~1-2]{Sutton2018}.

In this section we first introduce the elements of RL informally. We then formalise the RL Problem as the optimal control of incompletely-known Markov decision processes (finite? do I talk about PO?). We give a taxonomy of different RL solution methods and conclude with a description of one such method, Deep Q-Learning (DQN) that is of particular importance in this dissertation.

\section{Elements of Reinforcement Learning}
This subsection will introduce agent, environment, policy, reward signal, value function and [model] informally, similar to S\&B 1.3. Is this necessary or should I skip straight to the formalism?

\section{Finite Markov Decision Processes}
Finite Markov Decision Processes (finite MDPs) are a way of mathematically formalising the RL problem: they capture the most important aspects of the problem faced by an agent interacting with its environment to achieve a goal. We introduce the elements of this formalism: the agent-environment interface, goals and rewards, returns and episodes. Then...

\subsection{The Agent-Environment Interface}
MDPs consist firstly of the continual interaction between an agent selecting actions, and an environment responding by changing state, and presenting the new state to the agent, along with an associated scalar reward. Recall that the agent seeks to maximise this reward over time through its choice of actions.

More formally, consider a sequence of discrete time steps, $t = 1,2,3, \dots$. At each time step $t$, the agent receives some representation of the environment's \textit{state}, $ S_t \in \mathcal{S} $, and chooses an \textit{action}, $ A_t \in \mathcal{A} $. On the next time step, the agent receives reward $ R_{t+1} \in \mathcal{R} \subset \reals $, and finds itself in a new state, $ S_{t+1} $. These interactions repeat over time, giving rise to a \textit{trajectory}:
\begin{align*}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \dots
\end{align*}
% TODO insert Figure 3.1-like figure from S&B
% TODO ask best practices for figures
% TODO ask best practices for latex commands

A \textit{finite} MDP is one where the sets of states, actions and rewards are finite. In this case, the random variables $ S_t $ and $ R_t $ have well-defined discrete probability distributions which depend only on the preceding state and action. This allows us to define the \textit{dynamics} of the MDP, a probability mass function $ p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \mapsto [0,1] $, as follows. For any particular values $ s' \in \mathcal{S} $ and $ r \in \mathcal{R} $ of the random variables $ S_t $ and $ R_t $, there is a probability of these values occurring at time $ t $, given any values of the previous state $ s \in \mathcal{S} $ and action $ a \in \mathcal{A} $:
\begin{align*}
p(s', r \mid s, a) := \P{S_t = s', R_t = r \mid S_{t-1} = s , A_{t-1} = a }.
\end{align*}

A \textit{Markov} Decision Process is one where all states satisfy the Markov property. A state $ s_t $ satisfies this property iff:
\begin{align*}
\P{s_{t+1} \mid s_t, s_{t-1}, s_{t-2}, \dots, s_t } = \P{s_t}.
\end{align*}
This implies that in the MDP, the immediately preceding state $ s_t $ and action $ a_t $ are sufficient statistics for predicting the next state $ s_{t+1} $ and reward $ r_{t+1} $.
% I'm still slightly confused about Markov property being a restriction on states, and if I can write the equation \P(s_{t+1} \mid s_t, s_{t-1}, s_{t-2}, \dots, s_t ) = \P(s_t) given that I have only previously said that there is a well-defined probability function from state-action pairs to states and rewards

\subsection{Goals and Rewards}
The reader may have noticed that we first introduced MDPs as a formalism for an agent interacting with its environment to achieve a goal, yet have since spoken instead of maximising a reward signal $ R_t \in \reals $ over time. Our implicit assumption is the following hypothesis:
\begin{hypothesis}[Reward Hypothesis]
\textit{All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).} \cite[p.~53]{Sutton2018}
\end{hypothesis}
However, this hypothesis gives no information about how to construct such a scalar signal; only that it exists. Indeed, recent work has shown that it is far from trivial to do so; possible failure modes include negative side effects, reward hacking and unsafe exploration \cite{Amodei2016}. This is central to the topic of this dissertation---our aim is to improve the sample efficiency of one particular method of reinforcement learning when the reward signal is unknown.

\subsection{Returns and Episodes}
Having asserted that we can express the objective of reinforcement learning in terms of scalar reward, we now formally define this objective. Consider the following objective:
\begin{definition}[(Future discounted) return]
Let a sequence of rewards between time step $ t + 1 $ and $ T $ (inclusive) be $ R_{t+1}, R_{t+1}, \dots, R_T $. Let $ \gamma \in [0, 1] $ be a discount factor of future rewards. Then we define the (future discounted) return of this sequence of rewards \cite[p.~57]{Sutton2018},
\begin{equation} \label{G_t}
G_t := \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k.
\end{equation}
\end{definition}
This definition is convenient because it allows us to describe two kinds of reinforcement learning tasks, episodic and continuing, with the same notation. In episodic tasks, interactions between the agent and environment occur in well-defined subsequences, each of which ends in a special \textit{terminal state}. The environment then resets to a starting state, which may be fixed or sampled from a distribution. In continuing tasks, on the other hand, agent-environment interaction does not naturally divide into episodes.

For episodic tasks, it makes sense to consider (\ref{G_t}) in case $ T $ is the time step at which the episode ends, because we typically analyse either a single episode or something that holds across all episodes \cite[p.~57]{Sutton2018}. We call this \textit{episode} return. In this case, (\ref{G_t}) is well-defined regardless of the discount factor $ \gamma $. For continuous tasks, $ T = \infty $ thus (\ref{G_t}) may be infinite. However, if $ \gamma < 1 $ then the infinite sum (\ref{G_t}) will be finite in more cases. For example, if reward is a constant $ +r $, then the return is:
\begin{equation}
	G_t = \sum_{k=0}^{\infty} \gamma^k r = \frac{r}{1-\gamma},
\end{equation}
whereas the return would have been infinite if we did not use discounting.

\subsection{Policies and Value Functions}
\textit{Policy} determines the behaviour of the agent. Formally, a policy $ \pi : \mathcal{S} \times \mathcal{A} \mapsto [0,1] $ defines a probability distribution over actions, given a state. That is to say, $ \pi(a \mid s) $ is the probability of selecting action $ a $ if an agent is following policy $ \pi $ and in state $ s $.

The \textit{state-value function} $ v_\pi : \mathcal{S} \mapsto \reals $ for a policy $ \pi $ gives the expected return of starting in a state and following that policy. More formally,
\begin{definition}[State-value function]
	Let $ \pi $ be a policy and $ s \in \mathcal{S} $ be any state. We write $ \E{\pi}{.}$ to denote the expected value of the random variable $ G_t $ as defined in (\ref{G_t}). Then,
	\begin{equation} \label{v_pi}
	v_\pi(s) := \E{\pi}{G_t \mid S_t = s} = \E{\pi}{\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s}.
	\end{equation}
\end{definition}
% TODO work out how $\infty$ at the top of the summation jives with having T before... need to say that reward of terminal state is zero?

The \textit{action-value function} $ q_\pi : \mathcal{S} \times \mathcal{A} \mapsto \reals $ for a policy $ \pi $ is defined similarly. It gives the expected return of starting in a state, taking a given action, and following policy $ \pi $ thereafter.
\begin{definition}[Action-value function]
	Let $ \pi $ be a policy, $ s \in \mathcal{S} $ be any state and $ a \in \mathcal{A} $ any action. Then,
	\begin{equation} \label{v_pi}
	q_\pi(s, a) := \E{\pi}{G_t \mid S_t = s, A_t = a} = \E{\pi}{\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s, A_t = a}.
	\end{equation}
\end{definition}

%TODO recommence from p.59 SB

\subsection{Optimal Policies and Optimal Value Functions}


\subsection{Reinforcement Learning Solution Methods}
Reinforcement learning methods specify how an agent should use its experience update its policy $ \pi $.
% design choices; trade-offs?
% model-based v model-free
% Throw a deep NN at the problem!
% Current algos; SOTA on different tasks

\subsubsection{Deep Q-Learning}

% perhaps explain in detail just one algo (DQN, or whichever algo ends up being most important for my experiments)
% Question: do I need to describe neural networks too?

\section{Reinforcement Learning from Unknown Reward Functions}
For many domains in which we might want to use RL, states of the environment are not inherently associated with rewards.  Thus, we have the additional task of specifying a reward function, which maps states to rewards. Argue the case that directly specifying a reward function is hard, in order to motivate the next section: Reward Learning
\subsection{Reward Learning}
% We can use feedback of different modalities
% There is both the standard and the deep case to consider
\subsubsection{In Standard RL}
% Active Preference-Based Learning of Reward Functions (ignore the active part here)
% Learning Reward Functions by Integrating Human Demonstrations and Preferences (SAIL paper)
\subsubsection{In the Deep Case}
% SARM Agenda; Christiano; Ibarz

\chapter{Uncertainty in Deep Learning}
% Talk about Yarin's thesis, BNNs; maybe Ian Obsband paper for good measure (esp. if I end up using ensemble instead of MC-Dropout?)
Standard deep learning models output point estimates. For example, a model trained to classify pictures of dogs according to their breed takes a picture of a dog and outputs its predicted breed. However, what will the model do if it is given a picture of a cat? \cite{Gal2017a}.

We probably want the model to be able to recognise that this is an out of distribution example, and request more training data, or simply say that it doesn't know the answer. However, since standard deep learning models output only point estimates, the model will just go ahead and classify the cat as some breed of dog, just as confidently as any other input.

Thus, the field of Bayesian Deep Learning aims to equip neural networks with the ability to output a point estimate along with its uncertainty in that estimate. Historically, many of the attempts to do so were not very practical. For example, one algorithm, Bayes by Backprop, requires doubling the number of model parameters, making training more computationally expensive, and is very sensitive to hyperparameter tuning. However, recent techniques allow almost any network trained with a stochastic regularisation technique, such as dropout, to, given an input, obtain a predictive mean and variance (uncertainty), without any complicated augmentation to the network \cite[p.~15]{Gal2017a}.

\chapter{Active Learning}
% BALD (Houlsby: Bayesian Active Learning for Classification and Preference Learning)
% Adapting it to the deep case (Yarin's thesis/Image Data paper)
\subsection{Applying Active Learning to RL without a reward function}
% APRIL
% Christiano: summarise what didn't work, and what might have gone wrong (how to frame this depends on the results that I eventually get to)
% Active Preference-Based Learning of Reward Functions (emphasis on the active part)
% Learning Reward Functions by Integrating Human Demonstrations and Preferences (they use the `Volume Removal Method' to do Active Learning)

\chapter{Method}
% Here I'll describe the training protocol I used (mostly Ibarz but without the demos)
% I can leave gorey detail to Experiments/Appendix
% I should have already summarised the high level approach in Background
% And I'll describe the different methods of Active Learning/uncert estimates that I tried
% Implementational matters: PyTorch and why I chose it

\chapter{Experiments}
% Here I'll give the experimental details
% Gym. incl. why I chose it
% Cartpole/the envs I use
% Hyperparameter settings (and all the other args e.g. number of labels acquired per round, number of repetitions etc.) (maybe put these in appendix)
% Links to code

\chapter{Results}
% Here I'll describe the results in a shiny way
% Graphs and comments on whether I achieved the goal of applying active learning to increase the sample efficiency of reward modelling

\chapter{Conclusions}
\section{Summary}
\section{Evaluation}
% critical assessment of the work that has been done and the process of doing it
% perhaps include subsections for approaches that were tried and did not work; and personal development
\section{Future Work}

% acknowledgements

\bibliographystyle{plain}
\bibliography{MSc}

\appendix
\appendixpage
\noappendicestocpagenum
\addappheadtotoc
\chapter{Some Appendix Material}

\end{document}