\documentclass[11pt, a4paper, bibliography=totoc]{report}
\usepackage{amsmath,amsthm,amssymb, ltablex, listings, enumerate, cancel, bussproofs, tabto, wasysym, algpseudocode, algorithm, savesym, mathtools, physics, xcolor, setspace, appendix, dirtree, ragged2e, tikz}
\usepackage[margin=3.5cm]{geometry}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage[nottoc]{tocbibind} % adds Bibliography to TOCs
\usepackage[linktocpage]{hyperref}

\hypersetup{
	colorlinks,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}
\doublespacing
\pagestyle{headings}

% my math commands
\newcommand{\nats}{\mathbb{N}}
\newcommand{\reals}{\mathbb{R}}
\renewcommand{\P}[1]{\mathbb{P}\left( #1 \right) }
%\newcommand{\E}[1]{\mathbb{E} \left[ #1 \right] }
\newcommand{\E}[2]{\mathbb{E}_{#1} \left[ #2 \right] }
\newcommand{\V}[2]{\mathbb{V}ar_{#1} \left[ #2 \right]}
\newcommand{\KLD}[2]{\mathrm{KL} \left[ \left. \left. #1 \right|\right| #2 \right] }
\newcommand{\rp}{\hat{r}}
\newcommand{\expbuff}{\mathrm{E}}
\newcommand{\annbuff}{\mathrm{A}}
\newcommand{\repbuff}{\mathrm{D}}
\newcommand{\data}{\mathcal{D}}
\newcommand{\entropy}[1]{\mathbb{H} \left[ #1 \right] }
\newcommand{\MI}[1]{\mathbb{I} \left[ #1 \right] }
\newcommand{\w}{\mathbf{w}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\thetab}{\pmb{\theta}}

% these commands are left over from my BBB report; I might never use them
\newcommand{\vfe}{\mathcal{F}(\data, \theta)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\bepsilon}{\pmb{\epsilon}}
\newcommand{\bmu}{\pmb{\mu}}
\newcommand{\bsigma}{\pmb{\sigma}}
\newcommand{\btheta}{\pmb{\theta}}
\newcommand{\brho}{\pmb{\rho}}
\newcommand{\normal}[3]{\mathcal{N}(#1 \vert #2, #3)}

\newtheorem{claim}{Claim}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}

\begin{document}
\title{ARMA: Active Reward Modelling for Agent Alignment}
\author{Sam Clarke}
\date{September 2019}
\renewcommand{\bibname}{References}
\maketitle

\begin{abstract} % ~200 words

\end{abstract}

\tableofcontents
% say somewhere around here about collaboration with Zac and Angelos
% and where do I say who my supervisor is?

\part{Background}

\chapter{Introduction}

\section{Relation to Material Studied on the MSc Course}

\chapter{Reinforcement Learning} % Explain problem, give context
Reinforcement learning (RL) refers simultaneously to a problem, methods for solving that problem, and the field that studies the problem and its solution methods. The problem of RL is to learn what to do---how to map situations to actions---so as to maximise some numerical reward signal \cite[pp.~1-2]{Sutton2018}.

In this section we first introduce the elements of RL informally. We then formalise the RL Problem as the optimal control of incompletely-known Markov decision processes (finite? do I talk about PO?). We give a taxonomy of different RL solution methods and conclude with a description of one such method, Deep Q-Learning (DQN) that is of particular importance in this dissertation.

\section{Elements of Reinforcement Learning}
This subsection will introduce agent, environment, policy, reward signal, value function and [model] informally, similar to S\&B 1.3. Is this necessary or should I skip straight to the formalism?

\section{Finite Markov Decision Processes}
Finite Markov Decision Processes (finite MDPs) are a way of mathematically formalising the RL problem: they capture the most important aspects of the problem faced by an agent interacting with its environment to achieve a goal. We introduce the elements of this formalism: the agent-environment interface, goals and rewards, returns and episodes. Then...

\subsection{The Agent-Environment Interface} \label{subsection:agent_env_interface}
MDPs consist firstly of the continual interaction between an agent selecting actions, and an environment responding by changing state, and presenting the new state to the agent, along with an associated scalar reward. Recall that the agent seeks to maximise this reward over time through its choice of actions.

More formally, consider a sequence of discrete time steps, $t = 1,2,3, \dots$. At each time step $t$, the agent receives some representation of the environment's \textit{state}, $ S_t \in \mathcal{S} $, and chooses an \textit{action}, $ A_t \in \mathcal{A} $. On the next time step, the agent receives reward $ R_{t+1} \in \mathcal{R} \subset \reals $, and finds itself in a new state, $ S_{t+1} $. These interactions repeat over time, giving rise to a \textit{trajectory}, $ \tau $:
\begin{align*}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \dots
\end{align*}
% TODO insert Figure 3.1-like figure from S&B
% TODO ask best practices for figures
% TODO ask best practices for latex commands

A \textit{finite} MDP is one where the sets of states, actions and rewards are finite. In this case, the random variables $ S_t $ and $ R_t $ have well-defined discrete probability distributions which depend only on the preceding state and action. This allows us to define the \textit{dynamics} of the MDP, a probability mass function $ p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \mapsto [0,1] $, as follows. For any particular values $ s' \in \mathcal{S} $ and $ r \in \mathcal{R} $ of the random variables $ S_t $ and $ R_t $, there is a probability of these values occurring at time $ t $, given any values of the previous state $ s \in \mathcal{S} $ and action $ a \in \mathcal{A} $:
\begin{align*}
p(s', r \mid s, a) := \P{S_t = s', R_t = r \mid S_{t-1} = s , A_{t-1} = a }.
\end{align*}

A \textit{Markov} Decision Process is one where all states satisfy the Markov property. A state $ s_t $ of an MDP satisfies this property iff:
\begin{align*}
\P{s_{t+1}, r_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \dots, s_0, a_0 } = \P{s_{t+1} \mid s_t, a_t}.
\end{align*}
This implies that the immediately preceding state $ s_t $ and action $ a_t $ are sufficient statistics for predicting the next state $ s_{t+1} $ and reward $ r_{t+1} $.

\subsection{Goals and Rewards}
The reader may have noticed that we first introduced MDPs as a formalism for an agent interacting with its environment to achieve a goal, yet have since spoken instead of maximising a reward signal $ R_t \in \reals $ over time. Our implicit assumption is the following hypothesis:
\begin{hypothesis}[Reward Hypothesis]
\textit{All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).} \cite[p.~53]{Sutton2018}
\end{hypothesis}
However, this hypothesis gives no information about how to construct such a scalar signal; only that it exists. Indeed, recent work has shown that it is far from trivial to do so; possible failure modes include negative side effects, reward hacking and unsafe exploration \cite{Amodei2016}. This is central to the topic of this dissertation---our aim is to improve the sample efficiency of one particular method of reinforcement learning when the reward signal is unknown.

\subsection{Returns and Episodes} \label{rets_and_episodes}
Having asserted that we can express the objective of reinforcement learning in terms of scalar reward, we now formally define this objective. Consider the following objective:
\begin{definition}[(Future discounted) return]
Let a sequence of rewards between time step $ t + 1 $ and $ T $ (inclusive) be $ R_{t+1}, R_{t+1}, \dots, R_T $. Let $ \gamma \in [0, 1] $ be a discount factor of future rewards. Then we define the (future discounted) return of this sequence of rewards \cite[p.~57]{Sutton2018}:
\begin{equation} \label{G_t}
G_t := \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}.
\end{equation}
\end{definition}

One reason for introducing a discount factor is because we would like this infinite sum to converge. Accordingly, we impose the condition that $ \gamma < 1 $ whenever the reinforcement learning task is \textit{continuous}, that is to say, there may be an infinite number of non-zero terms in the sequence of rewards $ \{R_{t+1}, R_{t+2}, R_{t+3}, \dots \} $.

The other kind of task is called \textit{episodic}. Here, interactions between the agent and environment occur in well-defined subsequences, each of which ends in a special \textit{terminal state}. The environment then resets to a starting state, which may be fixed or sampled from a distribution. To adapt the definition in (\ref{G_t}) to this case, we introduce the convention that zero reward is given after reaching the terminal state. This is because we typically analyse such tasks by considering a single episode---either because we care about that episode in particular, or something that holds across all episodes \cite[p.~57]{Sutton2018}. Observe that summing to infinity in (\ref{G_t}) is then identical to summing over the episode, and that the sum is well-defined regardless of the discount factor $ \gamma $.

\subsection{Policies and Value Functions} \label{policy_value_functions}
\textit{Policy} determines the behaviour of the agent. Formally, a policy $ \pi : \mathcal{S} \times \mathcal{A} \mapsto [0,1] $ defines a probability distribution over actions, given a state. That is to say, $ \pi(a \mid s) $ is the probability of selecting action $ a $ if an agent is following policy $ \pi $ and in state $ s $.

The \textit{state-value function} $ v_\pi : \mathcal{S} \mapsto \reals $ for a policy $ \pi $ gives the expected return of starting in a state and following that policy. More formally,
\begin{definition}[State-value function]
	Let $ \pi $ be a policy and $ s \in \mathcal{S} $ be any state. We write $ \E{\pi}{.}$ to denote the expected value of the random variable $ G_t $ as defined in (\ref{G_t}). Then the state-value function (or simply, value function) for policy $ \pi $ is:
	\begin{equation} \label{v_pi}
	v_\pi(s) := \E{\pi}{G_t \mid S_t = s} = \E{\pi}{\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s}.
	\end{equation}
\end{definition}

The \textit{action-value function} $ q_\pi : \mathcal{S} \times \mathcal{A} \mapsto \reals $ for a policy $ \pi $ is defined similarly. It gives the expected return of starting in a state, taking a given action, and following policy $ \pi $ thereafter.
\begin{definition}[Action-value function]
	Let $ \pi $ be a policy, $ s \in \mathcal{S} $ be any state and $ a \in \mathcal{A} $ any action. Then the action-value function (or, Q-function) for policy $ \pi $ is:
	\begin{equation} \label{q_pi}
	q_\pi(s, a) := \E{\pi}{G_t \mid S_t = s, A_t = a} = \E{\pi}{\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s, A_t = a}.
	\end{equation}
\end{definition}

\subsection{Optimal Policies and Optimal Value Functions} \label{optimal_policy_value_functions}
%TODO I'm confused about optimal policy. On Spinningup it seems to be defined as the policy that maximises the v_\pi(s_0). But S&B say it's the policy that maximises v_\pi(s) for all s \in S. These two defs are inconsistent bc consider an unreachable state? Spinningup def could say that some policy is optimal despite being bad in unreachable state; S&B def would not...
% http://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-rl-problem
% p.64 S&B
**Some words here based on the TODO above.
The problem of Reinforcement Learning is thus to find an optimal policy.

All optimal policies share the same value functions. We call these the \textit{optimal state-value function}, $ v_* $, and the \textit{optimal action-value function} $ q_* $:
\begin{definition}[Optimal state-value function (from {\cite[p.~62]{Sutton2018}})]
	$$ v_*(s)  := \max_\pi v_\pi(s) ~~ \forall s \in \mathcal{S} .$$
\end{definition}
\begin{definition}[Optimal action-value function (from {\cite[p.~63]{Sutton2018}})]
	$$ q_*(s,a)  := \max_\pi q_\pi(s,a) ~~ \forall s \in \mathcal{S} ~~ \forall a \in \mathcal{A} .$$
\end{definition}

There is a simple connection between optimal Q-function and optimal policy that will be used in Section \ref{RL_solution_methods}:
\begin{claim} \label{Q_claim}
	If an agent has $ q_* $, then acting according to the optimal policy when in some state $ s $ is as simple as finding the action $ a $ that maximises $ q_*(s,a) $ \cite[p.~64]{Sutton2018}.
\end{claim}

\subsection{Bellman Equations}
These value functions obey special recursive relationships called Bellman equations. The equations are proved by formalising the simple idea that the value of being in a state is the expected reward of that state, plus the value of the next state you move to. Each of the four value functions defined in Sections \ref{policy_value_functions} and \ref{optimal_policy_value_functions} satisfy slightly different equations. We prove the Bellman equation for the value function and state the remaining three for completeness.

\begin{proposition}[Bellman equation for $ v_\pi $ {\cite[p.~59]{Sutton2018}}]
	Let $ \pi $ be a policy, $ p $ the dynamics of an MDP, $ \gamma $ a discount factor and $ v_\pi $ a state-value function. Then:
	\begin{align}
		v_\pi(s) = \underset{\substack{a \sim \pi(. \mid s) \\ s', r \sim p(., . \mid s, a) }}{\mathbb{E}} \left[ r + \gamma v_\pi(s') \right]
	\end{align}
\end{proposition}
\begin{proof}
	\begin{align*}
		v_\pi(s) &:= \E{\pi}{G_t \mid S_t = s} \\
		         &= \E{\pi}{\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s} \\
		         &= \E{\pi}{R_{t+1} + \gamma G_{t+1}} \\
		         &= \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{\substack{s' \in \mathcal{S} \\ r \in \mathcal{R}}} p(s', r \mid s, a) \left[ r + \gamma \E{\pi}{G_{t+1} \mid S_{t+1} = s'} \right] \\
		         &= \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{\substack{s' \in \mathcal{S} \\ r \in \mathcal{R}}} p(s', r \mid s, a) \left[ r + \gamma  v_\pi(s') \right] \\
		         &= \underset{\substack{a \sim \pi(. \mid s) \\ s', r \sim p(., . \mid s, a) }}{\mathbb{E}} \left[ r + \gamma v_\pi(s') \right]
	\end{align*}
\end{proof}

\begin{proposition}[Bellman equation for $ q_\pi $]
	Let $ \pi $ be a policy, $ p $ the dynamics of an MDP, $ \gamma $ a discount factor and $ q_\pi $ an action-value function. Then:
	\begin{align}
	q_\pi(s, a) = \underset{s', r \sim p(., . \mid s, a)}{\mathbb{E}} \left[ r + \gamma \underset{a' \sim \pi(.\mid s')}{\mathbb{E}}\left[q_\pi(s', a')\right] \right]
	\end{align}
\end{proposition}

\begin{proposition}[Bellman equation for $ v_* $ {\cite[p.~63]{Sutton2018}}]
	Let $ p $ be the dynamics of an MDP, $ \gamma $ a discount factor and $ v_* $ an optimal value function. Then:
	\begin{align}
	v_*(s) = \max_{a \in \mathcal{A}} ~ \underset{s', r \sim p(., . \mid s, a)}{\mathbb{E}} \left[ r + \gamma v_*(s') \right]
	\end{align}
\end{proposition}

\begin{proposition}[Bellman equation for $ q_* $ {\cite[p.~63]{Sutton2018}}] \label{bellman_q*}
	Let $ p $ be the dynamics of an MDP, $ \gamma $ a discount factor and $ q_* $ an optimal Q-function. Then:
	\begin{align}
	q_*(s, a) = \underset{s', r \sim p(., . \mid s, a)}{\mathbb{E}} \left[ r + \gamma ~ \underset{a' \in \mathcal{A}}{\max}\left[q_*(s', a')\right] \right]
	\end{align}
\end{proposition}

\section{Reinforcement Learning Solution Methods} \label{RL_solution_methods}
One method of solving the reinforcement learning problem is to explicitly solve a set of Bellman optimality equations. For example, in a finite MDP with $ n $ states and $ m $ actions, the Bellman equations for $ q_* $ are a set of $ n\cdot m $ equations in $ n\cdot m $ unknowns\footnote{This assumes that the agent can take any action in any state.}. Given the dynamics $ p $ of the MDP, standard techniques for solving systems of equations can be applied. Then, via Claim (\ref{Q_claim}), the agent has an optimal policy \cite[p.~64]{Sutton2018}.

However, in reality, we rarely have access to $ p $, or sufficient computational resources to solve this system of equations exactly \cite[p.~66]{Sutton2018}. Thus, much of the recent literature on RL solution methods focuses on finding approximate solutions.

In particular, there has been rapid development in a class of solution methods called \textit{deep reinforcement learning}. The idea is to use a \textit{deep neural network} to approximate some function that will yield an optimal policy. Typically, we approximate the optimal value function---this is called \textit{Q-learning}---or the optimal policy, directly---in which case it is termed \textit{policy optimization}\footnote{Some methods, such as DDPG \cite{lillicrap2015continuous}, TD3 \cite{fujimoto2018addressing} and SAC \cite{haarnoja2018soft} approximate both the optimal value function and the optimal policy.}.

In this section, we first provide background material on deep neural networks, necessary to understand how they approximate a Q-function or policy. We then explain in detail one particular deep reinforcement learning solution method, the deep Q-network, which is important for the rest of this thesis.

\subsection{Deep Neural Networks} \label{sec:dnn}
% model (func approx), data, loss, optimizer (mention batch GD, SGD, minibatch GD, then Adam and RMSProp briefly, since I use these)
% what is is relation between method of gradient descent and optimizer? where does the former fit into the  model, data, loss, optimizer thing? is it part of "optimizer"?
The goal of a deep neural network (NN) is to approximate some function $ f^* : \textbf{X} \mapsto \textbf{Y} $ \cite{Goodfellow-et-al-2016}. For example, in image classification, $ \textbf{X} $ may be image pixels, and $ \textbf{Y}Y $ some imge categories, for example, bird, plane or superhero. The neural network specifies a mapping $ y = f(\x ; \theta) $ that depends on some parameters $ \theta $. The task is then to learn the $ \theta $ that give the best approximation to the true function $ f^* $. A deep learning algorithm specifies how to do this.

As in machine learning in general, there are three key components to such an algorithm: model (or function approximator), cost function and optimization procedure andsubsubsection{Model}
The model, of course, is a deep NN. This simplest form of deep NN is a \textit{deep feedforward network}\footnote{They are also referred to as feedforward neural networks or multilayer perceptrons (MLPs)}. They are called feedforward because when the model is evaluated on input $ \x $, computation flows without ever feeding back into itself in a loop. They are called networks because it is natural to think of these models as being composed of many different functions, each of which is termed a \textit{layer}. Starting from the \textit{input layer}, the output of each layer flows into the next, through each of the \textit{hidden layers} until the \textit{output layer} is reaches and the function returns some value. Deep feedforward networks are deep inasmuch as they have many hidden layers \cite[p.~164]{Goodfellow-et-al-2016}.

\begin{example} \label{eg:nn}
Consider a very simple feedforward NN $ f : \reals^2 \mapsto \reals $ with one hidden layer. $ f $ is composed of three functions: $ f(\x) = f^{(3)}(f^{(2)}(f^{(1)}(\x))) $, where:
\begin{align*}
	f^{(1)}(\x) &= \mathbf{z} = \mathbf{W^1}\x \quad \text{for some $ 2\times 2 $ matrix $ \mathbf{W^1} $} \\
	f^{(2)}(\mathbf{z}) &= \mathbf{a} = \textup{ReLu}(\mathbf{a}) := \max\{0,\mathbf{a} \} \quad \text{where $ \max(.) $ is applied pointwise} \\
	f^{(3)}(\mathbf{a}) &= \mathbf{W^2}\mathbf{a} \quad \text{for some $ 1 \times 2 $ matrix $ \mathbf{W^2} $}
\end{align*}
\end{example}

\subsubsection{Cost function}
A \textit{cost function} (or \textit{loss function}) is some function that quantifies the performance of our model i.e. how close it is to the true function $ f^* $ we are trying to approximate. Most modern NNs use the \textit{negative log-likelihood} cost function\footnote{This is also referred to as the \textit{cross-entropy loss} function.} \cite[p.~138]{Goodfellow-et-al-2016}, which is simply the negative logarithm of the \textit{likelihood function}. Given a parametric family of probability distributions $ p_{\text{model}}(\x ; \theta) $ over the same space, a likelihood function gives the probability of a set of observations for different settings of the model parameters $ \theta $. More formally, consider a set of $ m $ i.i.d. examples $ \mathbb{X} = \{\x_1, \dots, \x_m \} $ drawn from a true but unknown data generating distribution. Then the likelihood function $ L(\theta) $ is defined by:
\begin{align*}
L(\theta) :=&~ p_{\text{model}}(\mathbb{X} ; \theta) \\
	=&~ \Pi_{i=1}^m p_{\text{model}}(\x_i ; \theta)
\end{align*}
Taking the logarithm of this function improves numerical stability, and taking its negative value is a convention because optimization in machine learning typically means \textit{minimizing} some function. Finally, we can divide by $ m $, which shows how we can interpret this function cost function as an expectation with respect to the empirical distribution $ \hat{p}_{\text{data}} $ defined by the training data $ \mathbb{X} $. Notice that minimising this modified function will give the same result and maximising the likelihood. So, we define the negative log likelihood cost function $ \text{NLL}(\theta) $ as
\begin{align} \label{eq:nll}
\text{NLL}(\theta) :=& -\frac{1}{m} \log L(\theta) \nonumber \\
=& -\frac{1}{m} \sum_{i=1}^m \log p_{\text{model}}(\x_i ; \theta) \nonumber \\
=&  -  \E{\x \sim \hat{p}_\text{data}}{\log p_{\text{model}}(\x ; \theta) } 
\end{align}
and seek the parameters $ \theta_{\text{ML}} $ which minimise this function:
\begin{align}
{\theta}_{\text{ML}} :=&{\arg\min}_{\theta} \text{NLL}(\theta) \nonumber
\end{align}

In this thesis, we will use $ \text{NLL}(\theta) $ cost function. Our model $ p_{\text{model}} $ is some neural network. Importantly, the NN output must define a probability distribution, which is implemented by making its final layer a softmax or Gaussian probability density function, for categorical and real-valued data, respectively. Furthermore, we actually use a slight generalisation of this procedure to estimate a conditional probability  $ p_{\text{model}}(\y \mid \x; \theta ) $ where $ \x \in \mathbf{X} $ are some inputs and $ \y \in \mathbf{Y} $ some outputs (also called \textit{targets}) of the function $ f^* : \textbf{X} \mapsto \textbf{Y} $ we are trying to approximate. This setting is called \textit{supervised learning}, because the training data is a set of supervised examples i.e. pairs of inputs and correctly labelled outputs. Finally, note that in the real-valued case, using a Gaussian density function on the output of a NN is equivalent to not passing the output values through the density function, and instead simply minimising $ \frac{1}{m} \sum_{i=1}^{m} ( \hat{y}_i - y_i ) $, the mean squared error between the inputs and targets  over the training data $ \langle (\x_i, y_i) \rangle_{i=1}^m $ where $ \hat{y_i} = f(\x_i) $, our model's prediction for input $ \x_i $ \cite[p.~132]{Goodfellow-et-al-2016}.

%Our data (as we shall see) will be generated by an RL agent acting in an environment, and by querying an oracle for preferences about that agent's behaviour. We will optimize the loss function using Adam.

%TODO do I use weight decay? If so, need to include this paragraph
%The cost function may include additional terms, for example those which add \textit{regularisation}. Such terms discourage the model parameters from simply memorising the training data, which would lead to poor generalisation to unseen examples. One simple example is including a \textit{weight decay} term, which modifies Equation \ref{eq:nll} to:
%\begin{equation} \label{eq:nllwd}
%\theta_{\text{ML}} := {\arg\min}_{\theta} ~ \lambda \left\lVert \theta \right\rVert^2_2 - \sum_{i=1}^{m} p_{\text{model}}(\x_i ; \theta)
%\end{equation}

\subsubsection{Optimization procedure}
The third component of a deep NN algorithm is the \textit{optimization procedure}. This specifies how we use the cost function to update the NN parameters $ \theta $. Since NNs of interest are typically non-convex functions (due to non-linear layers such $ f^{(2)} $ in Example \ref{eg:nn}), Equation \ref{eq:nll} cannot be optimized in closed form. Thus, we require some numerical optimization procedure \cite[p.~151]{Goodfellow-et-al-2016}; the most common is some variation of \textit{gradient descent}.

Gradient descent works by computing $ \nabla_\theta L(\theta) $, the partial derivate of some cost function $ L(\theta) $ with respect to the model parameters $ \theta $. Since the partial derivate of a function points in the direction of steepest ascent, selecting a new set of parameters $ \theta' = \theta - \epsilon ~ \nabla_\theta L(\theta) $ will mean that $ L(\theta') $ is less than $ L(\theta) $ for some small enough $ \epsilon $ \cite[p.~83]{Goodfellow-et-al-2016}. We can iteratively perform this procedure and eventually reach a \textit{local minimum} i.e. a point where $ L(\theta) $ is lower than at all neighbouring points. $ \epsilon $ is called the \textit{learning rate}; it is typically chosen by trying several small values and choosing that which results in the lowest final $ L(\theta) $.

If $ L(\theta) $ is non-convex then there is no guarantee that this point will be a \textit{global minimum} i.e. a point where $ L(\theta) $ takes its lowest possible value. Much machine learning research is concerned with modifications to this procedure, and how to set initial parameter values in order to avoid getting stuck in local minimum that lead to poor performance. As it turns out, finding local minima that are low enough often leads to very good performance.

One important modification is called \textit{stochastic gradient descent} (SGD). Notice that minimising Equation \ref{eq:nll} via gradient descent requires computing
\begin{align*}
\nabla_\theta \text{NLL}(\theta) = \frac{1}{m} \nabla_\theta \sum_{i=1}^{m} - \log p_{\text{model}}(\x_i ; \theta)
\end{align*}
on each iteration, which has computational complexity $ O(m) $ \cite[p.~149]{Goodfellow-et-al-2016}. For large training sets, this is infeasible.

SGD is based on the simple idea that this gradient is an expectation over the training data which we can approximate using a small sample \cite[p.~149]{Goodfellow-et-al-2016}:
\begin{equation}
\nabla_\theta \text{NLL}(\theta) \approx \frac{1}{m'}  \nabla_\theta \sum_{i=1}^{m'} - \log p_{\text{model}}(\x_i ; \theta)
\end{equation}
for some $ \mathbb{B} = \{\x_1, \dots, \x_{m'} \} \subset \mathbb{X} $ called a \textit{minibatch}, sampled uniformly at random from $ \mathbb{X} $.

In this thesis, we use a further variant of SGD called \textit{RMSProp} \cite{tieleman2012lecture}. This maintains individual learning rates for each model parameter and adapts them individually. Specifically, on each learning update, parameter $ \theta_i $ is rescaled according to the inverse square root of an exponentially weighted moving average of the historical squared values of the partial derivate of the cost function with respect to $ \theta_i $. This results in larger learning updates for parameters with small partial derivatives, and smaller learning updates for parameters with larger partial derivates. The intention is to converge to a minimum more rapidly than SGD. Algorithm \ref {alg:rmsprop} gives the precise procedure \cite[p.~304]{Goodfellow-et-al-2016}. 
\begin{algorithm}
	\caption{RMSProp.}
	\label{alg:rmsprop}
	\begin{algorithmic}[1]
		\Require Global learning rate $ \epsilon $, decay rate $ \rho $, initial parameters $ \theta $
		\State Initialise $ \mathbf{r} = 0 $ to accumulate squared gradients for each parameter
		\Repeat
		\State Sample minibatch $ \mathbb{B} = \{\x_1, \dots, \x_{m'} \} $
		\State Compute gradient $ \mathbf{g} \gets \frac{1}{m} \nabla_\theta \sum_i \text{NLL}(\x_i ; \theta) $
		\State Accumulate squared gradient $ \mathbf{r} \gets \rho \mathbf{r} + (1-\rho) {g} \odot \mathbf{g}  $
		\State Update parameters: $ \theta \gets \theta - \frac{\epsilon}{\sqrt{r + 10^{-6}}}\odot \mathbf{g} $
		\Until{convergence}
	\end{algorithmic}
\end{algorithm}
Note that the operations on lines 4-6 are all applied element-wise (that is, to each parameter individually). The adding $ 10^{-6} $ to $ r $ on line 6 improves numerical stability.

\subsection{Deep Q-network} \label{DQN}
We now have the necessary background to explain the deep Q-network (DQN). The idea is simply to use a deep NN to approximate the optimal Q-function $ q_* $ using a deep neural network $ Q(s,a; \theta) $ as the function approximator \cite{Mnih2015}. The agent-environment interactions yield experience $\langle (s_t, a_t, r_{t+1}, s_{t+1}) \rangle_{t=0}^T $ which can be used as training data. We can then simply perform gradient descent on the parameters $ \theta_i $ at iteration $ i $ to reduce the mean squared error between predictions $ Q(s,a; \theta_i) $ and targets given by the Bellman equation for $ q_*(s,a) $, from Proposition \ref{bellman_q*}. Since we do not have access to the true value of these targets, we instead use approximate target values $ y = r + max_{a'} ~ Q(s', a' ; \theta_i^-) $, with $ \theta_i^- $ some previous network parameters.

So far, this looks very similar to the supervised learning setting. However, there are three important differences that come with reinforcement learning. There are two sources of serrelations: both (i) in the data set, and (ii) between $ Q(s,a ; \theta_i) $ and the targets. AlsoFurti) updates to $ Q $ may change the policy and thus change the data distribution. This leads to instability in training. To address this, the authors propose two algorithmic innovations. Firstly, instead of training on experience in the order that it is collected, the agent maintains a buffer of experience $ D_t = \{ e_1, e_2, \dots, e_t \} $ where $ e_t = (s_t, a_t, r_{t+1}, s_{t+1}) $. When making learning updates, drawing minibatches uniformly at random from this buffer breaks correlations in the experience sequence and smooths over changes in the data distribution, alleviating problems (i) and (iii). This is termed \textit{experience replay}. Secondly, to reduce correlations between $ Q $ and the targets and alleviate problem (ii), the approximate target values are updated to match the parameters $ Q $ only every $ C $ steps for some hyperparameter $ C > 1 $.

With these changes, we arrive at a loss function $ \ell_i(\theta_i) $ for each learning update $ i $:
\begin{align*}
\ell_i(\theta_i) &= \E{ s,a,r }{ (\E{s'}{y \mid s,a} - Q(s,a;\theta_i))^2 } \\
                 &= \E{ s,a,r }{ \E{s'}{y - Q(s,a;\theta_i) \mid s,a } ^2 } \\
                 &= \E{ s,a,r }{ \E{s'}{(y - Q(s,a;\theta_i))^2 \mid s,a}  - \V{s'}{y - Q(s,a;\theta_i)) \mid s,a}   } \\
                 &= \E{s,a,r,s'}{(y - Q(s,a;\theta_i))^2} - \E{s,a,r}{\V{s'}{y}}
\end{align*}
where the expectations and variances are with respect to samples from the experience replay. This loss function is then optimized by stochastic gradient descent with respect to the network parameters $ \theta_i $. Note that the final term is independent of these parameters, so we can ignore it. Finally, the authors found that stability is improved by clipping the error term $ y - Q(s,a;\theta_i) $ to be between $ -1 $ and $ 1 $.

We summarise this training procedure in Algorithm \ref{alg:dqn}. To ensure adequate exploration, the agent's policy is $ \epsilon $-greedy with respect to the current estimate of the optimal action-value function.

\begin{algorithm}
	\caption{Deep Q-learning with experience replay.}
	\label{alg:dqn}
	\begin{algorithmic}[1]
		\State Initialise replay memory $ D $ to capacity $ N $
		\State Initialise neural network $ Q $ with random weights $ \theta $ as approximate optimal action-value function
		\State Initialise neural network $ \hat{Q} $ with identical weights $ \theta^- = \theta $ as approximate target action-value function
		\State Reset environment to starting state $ s_0 $
		\For{$ t=0, \dots, T $}
		\State With probability $ \epsilon $ execute random action $ a_t $
		\State otherwise execute action $ a_t = \arg\max_a Q(s_t, a ; \theta) $
		\State Observe next state and corresponding reward $ s_{t+1}, r_{t+1} \sim p(.,.\mid s_t,a_t) $
		\State Store transition $ (s_t, a_t, r_{t+1}, s_{t+1}) $ in $ D_t $
		\State Randomly sample minibatch of transitions $ (s_j, a_j, r_{j+1}, s_{j+1}) \sim D_t $
		\State Set $ y_j = \begin{cases}
		                   r_{j+1} \text{ if episode terminates at step $ j+1 $} \\
		                   r_{j+1} + \gamma \max_{a'} \hat{Q}(s_{j+1}, a_j ; \theta^- ) \text{ otherwise}
		\end{cases}$
		\State Do gradient descent on $ (y_j - Q(s_j, a_j ; \theta))^2 $ w.r.t network parameters $ \theta $
		\State Every $ C $ steps set $ \theta^- = \theta $
		\EndFor
	\end{algorithmic}
\end{algorithm}

Note that line 11 assumes we are training DQN to perform an episodic task, hence the first case which follows the convention given in Section \ref{rets_and_episodes} whereby zero reward is given for all states after the terminal state. If the task were instead continuing, line 11 would simply be $ y_j = r_{j+1} + \gamma \max_{a'} \hat{Q}(s_{j+1}, a_j ; \theta^- ) $.

\section{Reinforcement Learning from Unknown Reward Functions}
So far, we have assumed that as the agent interacts with the environment, it receives both information about the next state and the associated scalar reward. This presents an obvious challenge if we want to apply RL to solve real-world problems: since the world does not give scalar rewards, it seems we would have to manually specify a reward function mapping states of the world to rewards. For complex or poorly defined goals, this is difficult to do. If we try instead to design an approximate reward function, an agent optimizing hard for this objective will do so at the expense of satisfying our preferences \cite[p.~1]{Christiano2017}.

To circumvent this issue, a growing body of work studies how to do RL from various forms of human \textit{feedback}, instead of an explicit reward function. Three main feedback methods have been studied, all of which involve a human-in-the-loop providing information to the agent about the desired behaviour. Firstly, an agent may learn from expert demonstrations. This may involve using demonstrations to infer a reward function, an approach known as \textit{inverse reinforcement learning} \cite{Ng2000, Ziebart2008}. One can then use a standard RL algorithm on this recovered reward function. Other methods involve training a policy directly from demonstrations, referred to as \textit{imitation learning} \cite{Ho2016, Hester2017}.

Secondly, an agent may learn from feedback on its current policy in the form of scalar rewards \cite{Knox2009, Warnell2017}. Instead of providing a set of demonstrations, the human-in-the-loop observes the agent's behaviour and gives an appropriate reward. If it is assumed that the human provides this reinforcement according to some latent reward function $ \hat{r} : \mathcal{S} \times \mathcal{A} \mapsto \reals $, then standard supervised learning techniques can be applied to model this function. The agent can then select actions so as to maximise expected modelled reward. This approach differs from manually specifying a reward function because the human does not provide a precise function mapping all possible state-action pairs to rewards in advance. Rather, the human remains in the loop, providing reinforcement to the agent in an online fashion.

Finally, an agent may learn from binary preferences over trajectories \cite{Wilson2012, Christiano2017}. As with the \textit{policy feedback} method, the human-in-the-loop observes the agent's behaviour. However, instead of giving reinforcement in the form of scalar rewards, they are periodically presented with a pair of trajectories and must indicate which they prefer\footnote{The human also has the option of expressing indifference, or that the trajectories are incomparable.}. Given some assumptions about how the human's preferences relate to their latent reward function, we can again model $ \hat{r} $ by supervised learningI Then, standard RL algorithms can be applied to maximise the cted $ \rp $ over time.
%TODO highlight that Warnell doesn't do RL, but rather the agent just selects actions to maximise \rp? I'm confused about why this works. If you don't apply an RL algo, won't the agent be too myopic?
%TODO is this last paragraph too specfic since it only applies to the reward learning from trajectory preferences in the Deep RL case?

Each of these methods have shown promising results, and some of the advantages and disadvantages are summarised in Table \ref{table:1}.

\small\singlespacing\RaggedLeft
\begin{tabularx}{\textwidth}{ 
		|X|X|X|X|
	}
	\hline 
\textbf{Property} & \textbf{Trajectory preferences} & \textbf{Expert demonstrations} & \textbf{Policy feedback} \\ 
\hline 
Demandingness for human & Human only needs to judge outcomes & Human needs to perform task (expertly) & Human needs to provide suitable scalar rewards \\ 
\hline 
Upper bound on performance? & Superhuman performance is possible & Impossible to significantly surpass performance of expert & Superhuman performance is possible \\ 
\hline 
Suitability to exploration-heavy tasks & Limited\footnote{\label{footnote:exploration}The human can only give feedback on states visited by the agent. If the agent does not explore well, this limits the amount of information the human can convey. Since exploration is determined by the inferred reward func } & Well suited, since demonstrations can guide exploration & Limited\footnote{See footnote \ref{footnote:exploration}} \\ 
\hline 
Communication efficiency & On the order of hundreds of bits per human hour & Much richer in information than trajectory preferences \footnote{\cite{Ibarz2018} show that demonstrations half the amount of human time required to achieve the same level of performance} & Scalar rewards provide richer information than binary preferences over trajectories, but not as rich as demonstrations \\ 
\hline 
Computational efficiency (in simple Atari environments) & On the order of 10 million RL time steps & On the order of 10 million RL time steps & On the order of thousands of learning time steps \footnote{Note that the work which prototypes this method trains, with an unspecified amount of compute, a deep autoencoder to extract 100 features from the Atari game screen \textit{before} commencing the RL stage. The other methods do not do such pretraining, thus the reported results do not allow for a fair comparison.} \\ 
\hline
\caption{Summary of the properties of using different forms of human feedback in RL without a reward function.} \label{table:1}
\end{tabularx}
\normalsize\doublespacing\justify

Noticing that the properties on which learning from preferences performs poorly are precisely those on which learning from demonstrations performs well, it is intuitive to think that a combination of feedback methods will give better performance than either one individually. Recent work has confirmed this intuition. Specifically, \cite{Ibarz2018} test this method on nine Atari games \cite{bellemare2013atari}, and show that combined preferences and demonstrations outperform only demonstrations on eight games, and only preferences on the four exploration-heavy games\footnote{The contribution of demonstrations in games without difficult exploration is not significant, except for in two games where demonstrations are harmful compared to using only preferences. The authors hypothesise that this is due to the relatively poor performance of the expert \cite[p.~6]{Ibarz2018}}. \cite{Palan2019} show that in a 2D driving simulator \cite{Byk2017} and two OpenAI Gym environments \cite{brockman2016gym}, using just one expert demonstration reduces by a factor of 3 the number of human preferences required to achieve the same performance \cite[p.~6]{Palan2019}.

This dissertation concerns using active learning to improve the performance of reward learning from trajectory preferences. So far, work on feedback from preferences has taken two different approaches: training a reward model on handcrafted features of the environment, and taking the deep learning approach of training a reward model end-to-end without handcrafted features. As active learning has already been shown to improve performance in the former setting \cite{Byk2017}, we focus on applying active learning in the latter setting. Algorithmic innovation in this setting is also more exciting, because if we want to scale RL to real-world tasks, it is unlikely that we will be able to handcraft all the correct features.

In the remainder of this section, we explain in detail the algorithm used for the latter approach, which was developed in \cite{Christiano2017}. We then summarise briefly the difference in learning from preferences with handcrafted features, in which active learning has already been applied successfully.

\subsection{Reward Learning from Trajectory Preferences in Deep RL}

\subsubsection{Setting}
The agent-environment interface is as described in Section \ref{subsection:agent_env_interface}, with the modification that on step $ t $, instead of receiving reward $ R_{t+1} \in \mathcal{R} \subset \reals $, there is an \textit{annotator} who expresses preferences between \textit{trajectory segments}, or \textit{clips}. A clip is a finite sequence of states and actions $ ((s_0,a_0), (s_1,a_1),\dots,(s_{k-1},a_{k-1})) \in (\mathcal{S} \times \mathcal{A})^k $. If the annotator prefers some clip $ \sigma^1 $ to another clip $ \sigma^2 $, write $ \sigma^1 \succ \sigma^2 $. The annotator may also be indifferent between the clips, in which case we write $ \sigma^1 \sim \sigma^2 $. The agent does not see the annotator's implicit reward function $ r : \mathcal{S} \times \mathcal{A} \mapsto \mathcal{R} $, and must instead use the preferences expressed by the annotator to maximise $ r $ over time. This is the goal of reward learning from trajectory preferences.
%TODO need to define \succ in terms of r(.,.) ?
If the annotator could write down their true $ r $, then clearly we could perform traditional RL instead of taking the reward learning approach. However, as we noted above, we are interested in applying RL to tasks for which we do not have the true $ r $, which is when reward learning is useful. Nonetheless, for the purposes of quantitatively evaluating the method, we consider tasks for which we do have access to the true $ r $.

\subsubsection{Method}
The method has two components: a policy $ \pi : \mathcal{S} \times \mathcal{A} \mapsto [0,1] $ and an estimate of the annotator's reward function, $ \rp : \mathcal{S} \times \mathcal{A} \mapsto \mathcal{R} $, called a \textit{reward model} or \textit{reward predictor}. Both components are parametrised by a deep neural network. The agent learns to maximise the annotator's implicit reward function over time by iterating through the following three processes:
\begin{enumerate}
	\item Reinforcement learning by a traditional deep RL algorithm whereby policy $ \pi $ interacts with the environment for $ T $ steps and updates its parameters to optimise $ \rp $ over time. Agent experience $ \expbuff = ((s_0, a_0), (s_1, a_1),\dots, (s_{T-1}, a_{T-1}) ) $ is stored.
	\item Select pairs of clips $ (\sigma^1, \sigma^2) $ from $ \expbuff $, request a preference $ \mu $ from the annotator on each sampled pair, and add the labelled pair to the annotation buffer $ \annbuff $.
	\item Supervised learning to train the reward model on $ \annbuff $, the preferences expressed by the annotator so far.
\end{enumerate}
More detail on each process is provided below.

\subsubsection{Process 1: Training the policy}
As mentioned, process 1 is akin to traditional RL except $ \rp $ is used in place of the environment reward $ r $. There are two subtleties to mention. Firstly, since $ \rp $ is learned while RL is taking place, \cite{Christiano2017} prefer policy optimization methods over Q-learning, as these have been successfully applied to RL tasks with a non-stationary reward function \cite{Ho2016}. Specifically, they use A2C \cite{Mnih2016} and TPRO \cite{Schulman2015}. However, the follow up paper \cite{Ibarz2018} uses a Q-learning method\footnote{The reason for deviating from the recommendation in \cite{Christiano2017} is that \cite{Ibarz2018} combine reward learning from trajectory preferences and expert demonstrations, and DQfD is state-of-the-art for the latter problem.}, DQfD \cite{Hester2017}, and it is not clear that performance is impaired. Hence, the literature is ambiguous on whether a non-stationary reward function is necessarily problematic for Q-learning. %TODO link to the section of my method where I say that I used DQN; motivate why I did this; say that reinitialising agent should make it okay.

Secondly, since the reward model $ \rp $ is trained only on pairwise comparisons, its scale is underdetermined. Previous work therefore proposes periodically normalising $ \rp $ to have zero mean and constant standard deviation over the examples in $ \annbuff $. This is crucial for training stability since deep RL is sensitive to the scale of rewards. %TODO perhaps comment here or later on what Zac said that scaling rewards can actually in some cases change optimal policy...
%TODO comment here on later on the subtelties of normalisation when using an ensemble for your rewards.

\subsubsection{Process 2: Selecting and annotating clip pairs}
Previous work uses two methods for selecting clip pairs. \cite{Ibarz2018} sample uniformly at random from $ \expbuff $. \cite{Christiano2017} train an ensemble of three reward predictors and select the clip pairs with the maximum standard deviation across the ensemble. Roughly this favours clip pairs on which the model is most uncertain, with the hope of improving sample efficiency (that is to say, being able to learn $ \rp $ with fewer labels from the annotator). However, they found that relative to random selection, this sometimes impaired performance and slowed down training. In Section ** we consider what caused this.

The selected clip pairs $ \sigma^1, \sigma^2 $ are then annotated with a label $ \mu $, indicating which clip is preferred. $ \mu $ is a distribution over $ \{1,2\} $ where $ \mu(1) = 1, \mu(2) = 0 $ if $ \sigma^1 \succ \sigma^2 $; $ \mu(1) = 0.5, \mu(2) = 0.5 $ if $ \sigma^1 \sim \sigma^2 $; or $ \mu(1) = 0, \mu(2) = 1 $ if $ \sigma^2 \succ \sigma^1 $. The triples $ (\sigma^1, \sigma^2, \mu) $ are then added to $ \annbuff $. The majority of previous work uses a \textit{synthetic annotator} rather than an actual human. Labels are simply generated according to the (hidden) ground truth reward function $ r $, where $ \sigma^1 \succ \sigma^2 $ if $ \sum_t r(s_t^1, a_t^1) > \sum_t r(s_t^1, a_t^1) $; $ \sigma^1 \sim \sigma^2 $ if $ \sum_t r(s_t^1, a_t^1) = \sum_t r(s_t^1, a_t^1) $; and $ \sigma^2 \succ \sigma^1 $ otherwise. This facilitates quicker experimentation and more clear performance metrics (by using hidden ground truth reward function to evaluate agent performance and reward model alignment).

\subsubsection{Process 3: Training the reward model}
The training of $ \rp $ is based on the following assumption:
\begin{assumption} \label{assumption:1}
	The annotator's probability of preferring clip 1 to clip 2, $ \hat{P}(\sigma^1 \succ \sigma^2) $, depends exponentially on the value of $ \hat{r} $ summed over the clips.
\end{assumption}
This allows us to write:
\begin{equation} \label{eq:softmax}
\hat{P}(\sigma^1 \succ \sigma^2 ; \rp) = \frac{\exp \sum_t \hat{r}(s_t^1, a_t^1)}{\exp \sum_t \hat{r}(s_t^1, a_t^1) + \exp \sum_t \hat{r}(s_t^2, a_t^2)}
\end{equation}
We can then fit the parameters of $ \rp $ by treating the problem as binary classification. In other words, we can use standard supervised learning techniques to optimize the parameters of $ \rp $ so as to minimise the cross-entropy loss between the predictions in \ref{eq:softmax} and the annotator's labels.
\begin{equation} \label{eq:loss}
\text{loss}(\hat{r}) = -\sum_{(\sigma^1, \sigma^2, \mu) \in \annbuff} \mu(1) \log \hat{P}(\sigma^1 \succ \sigma^2 ; \rp) + \mu(2)\log \hat{P}(\sigma^2 \succ \sigma^1 ; \rp)
\end{equation}
Assumption \ref{assumption:1} follows the Elo rating system developed for chess \cite{elo1978rating}. Given a zero-sum game and two players with a scalar rating, Elo specifies a mapping from player ratings (in our case: reward) to the probability of each player winning (in our case: the probability of each clip being preferred by the annotator).

\subsection{Reward Learning from Trajectory Preferences with Handcrafted Feature Transformations}
\subsubsection{Setting}
\cite{Byk2017} consider a more narrow setting. There are two vehicles, $ H $ which is ``human driven'', and $ R $ which is a ``robot''. The vehicles are in a 2D environment with each obeying a simple point-mass dynamics model, with state space $ \mathcal{S} $:
\begin{align*}
[ \dot{x}~~~ \dot{y}~~~ \dot{\theta} ~~~ \dot{v} ] = [  v \cdot \cos(\theta) ~~~ v \cdot \sin(\theta) ~~~ v \cdot u_1 ~~~ u_2 - \alpha\cdot v  ] 
\end{align*}
where $ v $ and $ \theta $ are vehicle velocity and direction respectively. The action space $ \mathcal{A} $ is $ [u_1, u_2] $ which represent steering and acceleration respectively. $ \alpha $ is a friction coefficient.

They assume that $ \hat{r} : \mathcal{S} \times \mathcal{A} \mapsto \reals $ is a linear combination of a set of five features:
\begin{align*}
\rp(s,a) = \w^T \phi(s,a).
\end{align*}
These five features $ \w = [w_1, w_2, w_3, w_4, w_5] $ are handcrafted, and correspond to (i) distance to road boundary, (ii) distance to centre of road lane with an extra penalty for shifting lane, (iii) difference between vehicle speed and the speed limit, (iv) dot product between $ \theta $ and a vector pointing along the road, and (v) a non-spherical Gaussian over the distance from $ R $ to $ H $ (to penalise collisions).

Like \cite{Christiano2017}, there is annotator who is queried for preferences on trajectory segments. Some $ \w_\text{true} $ is specified in advance, and the annotator uses this to answer the queries as in Process 2 of \cite{Christiano2017}.

\subsubsection{Method}
\cite{Byk2017} do not attempt to train a policy using the learned reward model. Their performance metric is simply the expected similarity of the true and predicted feature weights:
\begin{align*}
m = \mathbb{E}\left[  \frac{\w \cdot \w_\text{true}}{\vert \w \vert \vert \w_\text{true} \vert}  \right]
\end{align*}
They generate queries by solving a constrained optimization problem to find the two trajectory segments that will maximise the volume removed from the hypothesis space. Using the same mapping from reward space to preference space as \cite{Christiano2017} (Equation \ref{eq:softmax}), they start with a uniform prior over the space of all $ \w $ (uniform over the unit ball), generate a query, get a preference from the annotator, and perform a Bayesian update on this labelled clip pair. Thanks to the simple function form of their reward model, this Bayesian update has a closed form solution. They repeat this procedure until the reward model is close to the true reward function, according to $ m $.

\chapter{Uncertainty in Deep Learning}
When using a deep NN for almost any practical application, it is undesirable to 
As we saw in Section \ref{sec:dnn}, deep NNs output point estimates. %TODO we acutally said they ouput probabilities, so I need to say Yarin's thing about them not truly capturing uncertainty instead
For example, a model trained to classify pictures of dogs according to their breed takes a picture of a dog and outputs its predicted breed. However, what will the model do if it is given a picture of a cat? \cite{Gal2017a}.

We probably want the model to be able to recognise that this is an out of distribution example, and request more training data, or simply say that it doesn't know the answer. However, since standard deep learning models output only point estimates, the model will just go ahead and classify the cat as some breed of dog, just as confidently as any other input.

Thus, the field of Bayesian Deep Learning aims to equip neural networks with the ability to output a point estimate along with its uncertainty in that estimate. Historically, many of the attempts to do so were not very practical. For example, one algorithm, Bayes by Backprop, requires doubling the number of model parameters, making training more computationally expensive, and is very sensitive to hyperparameter tuning. However, recent techniques allow almost any network trained with a stochastic regularisation technique, such as dropout, to, given an input, obtain a predictive mean and variance (uncertainty), without any complicated augmentation to the network \cite[p.~15]{Gal2017a}.

\section{Bayesian Neural Networks} \label{sec:bnns}
From Andreas:
2.3 Bayesian Neural Networks (BNN)
In this paper we focus on BNNs as our Bayesian model because they scale well to high dimensional
inputs, such as images. Compared to regular neural networks, BNNs maintain a distribution over
their weights instead of point estimates. Performing exact inference in BNNs is intractable for any
reasonably sized model, so we resort to using a variational approximation. Similar to Gal et al. [10],
we use MC dropout [9], which is easy to implement, scales well to large models and datasets, and is
straightforward to optimise.

%Mention BBB as another method of VI, and that ensemble of models is another way to sample from approximate posterior %TODO what is the citation for this?t
Explain what VI is by lifting words from your BBB report.

\section{Model Uncertainty in BNNs}
This section would say basically the same stuff as active learning, because the acquisiiton functions we consider are precisely the ways to get model uncertainty with a BNN in classification setting.
Not sure if I need to include it, or maybe I should collapse the sections.

\chapter{Active Learning}
% BALD (Houlsby: Bayesian Active Learning for Classification and Preference Learning)
% Adapting it to the deep case (Yarin's thesis/Image Data paper)
Supervised learning requires labelled training data. However, for many real-world problems, obtained labelled data is expensive. For example, constructing a dataset for image classification requires a human to specify the category of every image in training data. Active learning \cite{cohn1996active} is a framework for training a model using less data to achieve the same performance, by acquiring only the data that is most informative to the model. The key ingredient in active learning is called an \textit{acquisition function}. Given a model $ \mathcal{M} $ and pool data $ \mathcal{D}_{pool} \subset X $, an acquisition function $ a : X \times \mathcal{M} \mapsto \reals $ quantifies how informative the label of an element $ \x \in \mathcal{D}_{pool} $ would be to the model. This determines the next $ \x $ to query the \textit{oracle} for a label. More precisely, we acquire each new training datum $ x^* $ according to \cite{Gal2017b}:
\[ \x^* = {\arg\max}_{\x \in \mathcal{D}_{pool}} a(\x, \mathcal{M}) \]
Acquisition functions are often based on uncertainty estimates, corresponding with the intuition that it is good to acquire data on which the model is currently uncertain. In this section, we review some common acquisition functions for classification tasks\footnote{Different acquisition functions are used for regression tasks, typically based on predictive variance \cite[p.~47]{Gal2017a} which I do not cover here.} and then discuss how BNNs can provide the uncertainty estimates that these functions require. %TODO well in the Model Uncertainty in BNNs section I probably say stuff about this. work out what more, if anything, i need to say here.

\section{Acquisition Functions} \label{sec:acq_funcs}

\subsection{Max Entropy} \label{sec:max_ent}
One natural idea is to acquire $ \x \in \data_{pool} $ with the highest entropy \cite{shannon1948mathematical} given the current training data $ \data_{train} $. Entropy is a key concept in information theory, which is a mathematical formalisation of uncertainty. More specifically, given a predictive distribution $ p : X \times Y \mapsto [0,1] $ (i.e. a model which predicts the probability of input $ \x $ being in class $ y $) trained on dataset $ \data_{train} $, the \textit{predictive entropy} of a new input $ \x $ formalises how uncertain $ p $ is about the label of input $ \x $ \cite[p.~52]{Gal2017a}:
\begin{equation}\label{eq:entropy}
\entropy{y \mid \x, \data_{train}} = -\sum_{c} p(y=c \mid \x, \data_{train} ) \log p(y=c \mid \x, \data_{train} )
\end{equation} %TODO I'm a little confused about when we should use mathbb{P}(.) and when we use p(.)
where the sum is over the possible classes $ c $ of label $ y $.

%TODO it would be better to set up a running example for this section; the presentation currently is a little clunky; I have to keep referring to the same thing.
\begin{example} \label{ex:max_ent}
	Consider the binary classification case where we have two classes i.e. $ c \in \{0, 1\} $. Given some input $ \x $, if the model $ p $ predicts $ 0.5 $ for both classes, i.e. $ p(y=0 \mid \x, \data_{train}) = p(y=1 \mid \x, \data_{train}) = 0.5 $ then $ \entropy{y \mid \x, \data_{train}} $ will take its maximum value of $ \log(2) $, corresponding with the intuition that the model is maximally uncertain as to the label of $ \x $ because it predicts label $ 0 $ and label $ 1 $ with equal probability. The other extreme is when the model predicts exactly $ 0 $ or $ 1 $ for the label of $ \x $. In this case $ \entropy{y \mid \x, \data_{train}} = 0 $. The model is already certain about the label of $ \x $ (and it would be pointless to acquire its label).
\end{example}
%TODO do I need to say slightly more precisely how we eval acquisition function? I think I'll do this below, because that's about BNNs (MC-Dropout, ensemble etc)

\subsection{BALD}
Whilst acquiring points by maximising entropy seems like a reasonable idea, one might wonder whether the data on which the model is the most uncertain are actually the most informative data to acquire. Consider that the pool might contain data which are inherently ambiguous, like a handwritten digit that is a borderline case between a $ 1 $ and a $ 7 $. It might not be very helpful to acquire such points, because their labels do not actually resolve any uncertainty. Such data are said to have high \textit{aleatoric uncertainty}, which is to say uncertainty due to noise inherent in the data, which cannot be resolved given more data \cite[p.~7]{Gal2017a}. Even if we had a many labels of $ 7 $'s that look like $ 1 $'s, or $ 1 $'s that look like $ 7 $'s, given a new ambiguous $ 1 $ or $ 7 $, we will still (correctly) be uncertain about its label.
%TODO I'm no longer convinced that BALD magically avoids inherently ambiguous data. I think BALD won't avoid such that the first time it sees it, because it seems reasonable to think that such data migh have predicted labels 0,1,0,1. But then, if it's acquired, it should go to 0.5, 0.5, 0.5 ... because all the models will agree on the ambiguity. but then, what if such data keeps coming in i.e. there is enough of it s.t. very often it will be acquired? Yeah, I'm more confused about this intuition than I thought

On the contrary, it seems better to acquire data which the model is uncertain about, but which also help to resolve uncertainty. Such data have high \textit{epistemic} or \textit{model uncertainty}. We claimed that predictive entropy as in Equation \ref{eq:entropy} represents a model's total uncertainty in its prediction. Total uncertainty comprises both that arising from noisy data, and uncertainty about model parameters and class. In other words, predictive uncertainty is sum of aleatoric and epistemic uncertainty.

Now, how can we specify an acquisition function that selects data which have high epistemic but low aleatoric uncertainty? For reasons which will soon become clear, we denote epistemic uncertainty as $ \MI{y, \w \mid \x, \data_{train}} $ where $ \w $ are the parameters of our model. This is called \textit{mutual information} between the prediction \textit{y} and the model parameters $ \w $. Since we now consider uncertainty in both noisy data and the model parameters, we need some extra notation. Consider our model parameters $ \w \sim p(.\mid \data_{train}) $ to be sampled from a distribution over model parameters, which depends on the dataset on which the model has been trained. Now, we can write aleatoric uncertainty as expected predictive entropy, where the expectation is over draws of our model parameters: $ \E{p(\w \mid \data_{train})}{ \entropy{y \mid \x, \w }} $. This formalises the intuition that aleatoric uncertainty is high if, even when model uncertainty is removed (because we consider only a \textit{single} draw of the model parameters), predictive entropy is still high. In this case, the uncertainty can only be coming from noise in the data. Conversely, if we take a single draw of the model parameters, leaving only aleatoric uncertainty, and predictive entropy is low, then aleatoric uncertainty must be low. %TODO I think I could say the thing and its converse in one and make these few sentences clearer. Also I think I'm still a little confused myself which seems bad.
%TODO I think I'm also a bit confused about the status of all the things I'm saying... claims (proved or unproved?), definitions, intuitions, cited stuff ? And also confused about whether this presentation of BALD, which I made up based on my understanding, is formally correct.

Putting this all together, we arrive at a formalisation of epistemic uncertainty, in terms of the difference between predictive and aleatoric uncertainty \cite[p.~53]{Gal2017a}:
\begin{align}\label{eq:bald}
\MI{y ; \w \mid \x, \data_{train}} &= \entropy{y \mid \x, \data_{train}} - \E{p(\w \mid \data_{train})}{ \entropy{y \mid \x, \w }} \nonumber \\
&= -\sum_{c} p(y=c \mid \x, \data_{train} ) \log p(y=c \mid \x, \data_{train} ) \nonumber \\ 
&\qquad + \E{p(\w \mid \data_{train})}{\sum_{c} p(y=c \mid \x, \w ) \log p(y=c \mid \x, \w )}
\end{align}

There is an alternative way to arrive at this formalisation of epistemic uncertainty, which is the reason for denoting it as $ \MI{y, \w \mid \x, \data_{train}} $. The mutual information between two random variables $ \MI{X ; Y} $ quantifies the information gained about $ X $ by observing $ Y $. Thus, $ \MI{y, \w \mid \x, \data_{train}} $ quantifies the information gained about the model parameters by observing the label $ y $ of input $ \x $, given the current training data $ \data_{train} $, which sound like a good metric for an acquisition function. By the definition of mutual information we can arrive at the same result as in Equation \ref{eq:bald}:
\begin{align*}
\MI{y ; \w \mid \x, \data_{train}} :&= \entropy{y \mid \x, \data_{train}} - \E{p(\w \mid \data_{train})}{ \entropy{y \mid \x, \w }}
\end{align*} %TODO add a step here -- I'm confued about exactly what the rearrangemen is. Where does the expectation come from? (see my notes on bottom of pp.15 Christiano. The orginal BALD paper just says this is "easy to show"!)
Using this objective as an acquisition function was first proposed in \cite{Houlsby2011}. Their intuition that it seeks to acquire examples on which particular settings of the model parameters are highly confident, but in disagreement with each other, or in the language used above, on which aleatoric uncertainty is low but epistemic uncertainty is high. They called this objective Bayesian Active Learning by Disagreement (BALD).

\begin{example}
	Again, consider the binary classification setting. In the second case presented in Example \ref{ex:max_ent} when individual draws of the model parameters always predict either $ 0 $ or $ 1 $, then (as with Max Entropy), $ \MI{y ; \w \mid \x, \data_{train}} = 0 $. There is no disagreement on the label of $ \x $ between different draws of the model parameters, and so the epistemic uncertainty is zero. However, in the first case, where individual draws of the model parameters always predict $ 0.5 $, then we get $ \MI{y ; \w \mid \x, \data_{train}} = \entropy{y \mid \x, \data_{train}} - \E{p(\w \mid \data_{train})}{ \entropy{y \mid \x, \w }} = \log(2) - \log(2) = 0 $. While this input has high predictive entropy, its label would not be very informative to the model, because the predictive entropy is due entirely to noise (which cannot be explained away given more data) rather than epistemic uncertainty. For an example which will score highly with respect to BALD, consider if sequential draws of the model parameters predict the label of some input $ \x $ to be $ 0, 1, 0, 1, \dots $. Then $ \MI{y ; \w \mid \x, \data_{train}} = \log(2) - 0 = \log(2) $, corresponding with the high confidence, but high disagreement between different model parameters.
\end{example}

\subsection{Variation Ratios}
%TODO in AL image data paper, they present the below (which is what I implement). However, in his thesis, Yarin presents a version which can be seen as approximating the below. It's a pedantic point, but why the difference?
Maximising the Variation Ratios \cite{freeman1965elementary} is similar to Max Entropy in that it seeks the $ \x $ on which the model has high predictive uncertainty. The difference is that it does not have an information theoretic formalisation. Instead,
\begin{align*}
\text{variation-ratio}[\x] := 1 - \max_y p(y \mid \x, \data_{train})
\end{align*}
Observe that this metric will be low in the second case presented in Example \ref{ex:max_ent}, because for the class $ y $ that is always predicted by the model, $ p(y \mid \x, \data_{train} = 1 $ and so $ \text{variation-ratio}[\x] = 0 $. Conversely, it will achieve its maximum value of $ 0.5 $ (in the binary setting) in the first case in Example \ref{ex:max_ent}, because $ p(y=0 \mid \x, \data_{train} = p(y=1 \mid \x, \data_{train} = 0.5 $. Thus, it is open to the same failure mode as Max Entropy: acquiring data with high aleatoric but low epistemic uncertainty.

\subsection{Mean STD}
Finally, an approach with less theoretical grounding that is nonetheless used in the literature is to acquire points that maximise the mean standard deviation $ \sigma(\x) $, where the mean is taken over the different classes $ c $ that input $ \x $ can take \cite{kampffmeyer2016semantic, kendall2015bayesian}.
\begin{align*}
\sigma_c &= \sqrt{\E{p(\w \mid \data_{train})}{p(y=c \mid \x, \w)^2} - \E{p(\w \mid \data_{train})}{p(y=c \mid \x, \w)}^2 } \\
\sigma(\x) &= \frac{1}{C} \sum_c \sigma_c
\end{align*}
This acquisition function has similar properties to BALD in that it standard deviation, like disagreement, will avoid data with high aleatoric and low epistemic uncertainty. This is easy to see: if different draws of the model parameters predict the label of input $ \x $ to be always $ 0.5 $, as in the first case in example \ref{ex:max_ent}, the standard deviation of these samples is zero.

\section{Applying Active Learning to RL without a reward function}
Discussion of previous work.

\subsection{APRIL}
% APRIL

\subsection{Active Preference-Based Learning of Reward Functions with handcrafted feature transformations}
% Summarise the active learning method used in 2017 papers by Sadigh's lab
%(they use the `Volume Removal Method' to do Active Learning)

\subsection{Deep RL from Human Preferences}
% Christiano: summarise what didn't work, and                                                                       
% what might have gone wrong -- already written this; just mention that i'll explain it in Section \ref{sec:failure}


\part{Innovation}

\chapter{Method}
% Here I'll describe the training protocol I used (mostly Ibarz but without the demos)
% I can leave gorey detail to Experiments/Appendix
% I should have already summarised the high level approach in Background
% And I'll describe the different methods of Active Learning/uncert estimates that I tried

\section{Possible failure modes of active reward modelling} \label{sec:failure}
Before developing our method, it was important to consider the possible failure modes of the previous attempt to apply active learning to reward modelling. \cite{Christiano2017} compute the standard deviation of each clip pair $(\sigma^1, \sigma^2)$ across the ensemble. In the language of acquisition functions, they use $ a_{mean\_std}((\sigma^1, \sigma^2), \rp) $, which in this setting is written as:
\begin{align*}
a_{mean\_std}((\sigma^1, \sigma^2), \rp) = \sqrt{\frac{1}{3}\sum_{i=1}^{3}\left(\hat{P_i}(\sigma^1 \succ \sigma^2  ; \rp) - \overline{\hat{P}(\sigma^1 \succ \sigma^2  ; \rp)}\right)^2},
\end{align*}
%TODO the 1/3 and the sqrt look kinda ugly. is there a better but still precise presentation of this?
where $ \hat{P_i}(\sigma^1 \succ \sigma^2  ; \rp) $ is the prediction $ \hat{P}(\sigma^1 \succ \sigma^2  ; \rp) $ according to component $ i $ of the ensemble, and $ \overline{\hat{P}(\sigma^1 \succ \sigma^2  ; \rp)} $ is the average of these predictions.

The quality of uncertainty estimates and acquisition function is one possible failure mode. Indeed, \cite[p.~6]{Christiano2017} explain the failure of their implementation as due to $ a_{mean\_std}((\sigma^1, \sigma^2), \rp) $ across an ensemble of three networks as a ``crude approximation'' to reward predictor uncertainty. Possible solutions to this failure mode are to use a different acquisition function (such as BALD \cite{Houlsby2011}), or to derive uncertainty estimates using a different method (such as MC-Dropout \cite{Gal2015} or Bayes by Backprop \cite{Blundell2015}).
%TODO insert citation Gal finds mean-std + MC-dropout cannot beat random (Figure 1 of AL Image paper)

Additionally, there are three features of the application of active learning to reward modelling that are not present in the standard supervised learning case. Firstly, the pool dataset is not known in advance, but is gathered as the RL agent encounters new states. This seems to present an exploration problem: naively, the agent's exploration is guided by the current reward function estimate, $ \rp $. Thus, the agent may not explore novel states and collect diverse clip pairs for the pool dataset. If the pool dataset is not diverse, it may be hard to beat the random acquisition baseline with active learning, since the data may all be more or less equally informative.
%TODO do I need to define diverse more rigorously

Secondly, since the objects that we acquire are \textit{clip pairs} rather than, for example, single images, yet the (growing) dataset is comprised of single clips, evaluating the acquisition function over the dataset is a complexity $ O(n^2) $ operation, for $ n $ the number of clips acquired. Therefore, the standard active learning procedure of evaluating the acquisition function on each point in the pool dataset and picking that which maximises it, quickly becomes unfeasible as the dataset grows in size. To get around this issue, in order to acquire $ k $ clip pairs, \cite{Christiano2017} randomly sample $ 10k $ clip pairs and select the $ k $ with the highest score according to $ a_{mean\_std} $. However, it is not clear that this method suffices to find clip pairs that are more informative than random acquisition: a factor of 10 may simply be too small.

Thirdly, it is not clear that the standard acquisition functions can be applied out of the box to learning \textit{in the preference space}. Consider the following example: we are deciding whether to acquire clip pair $ c_1 = (\sigma^1, \sigma^2) $ or $ c_2 = (\sigma^3, \sigma^4) $ to acquire. Suppose further that the reward model is uncertain whether $ c_1 $ has label $ 0 $ or $ 0.5 $\footnote{TODO I may need to spell this out more, in terms of draws from the posterior giving something like 0, 0.5, 0, 0.5 ... Can I borrow Yarin's presentation in thesis of a similar case?}; and uncertain whether $ p_2 $ has label $ 0 $ or $ 1 $. Now, $ a_{mean\_std}(c_1, \rp) =  $, whereas $ a_{mean\_std}(c_2, \rp) =  $, and thus we will acquire $ c_2 $. Yet, when learning in the preference space, using disagreement between models in an ensemble as the basis for an acquisition function may not capture all that we care about. It may be important, for example, to acquire clip pairs that allow the model to make deductions based on transitivity of the preference relation. For suppose that we have already acquired some clip pair $ (\sigma^0, \sigma^1) $. Then acquiring $ (\sigma^1, \sigma^2) $ would in effect give for free the label of $ (\sigma^0, \sigma^2) $, whereas the acquisition of $ (\sigma^3, \sigma^4) $ would not. Thus, we may need a better proxy than simply disagreement for active learning in the preference space.
%TODO I'm highly uncertain about whether this point is correct

There are other possible failure modes that apply to active learning in general. When performing \textit{batch acquisition}, that is, acquiring the top $ b $ points that maximise an acquisition function \cite{Gal2017b}, may lead to the acquisition of points that are informative individually, but jointly are much less informative than the sum of their parts. In particular, the acquisitions may not be very diverse. Indeed, \cite[p.~8]{Kirsch2019a} show that with acquisition size 5, BALD underperforms random acquisition on the EMNIST image dataset \cite{cohen2017emnist} when acquiring new images with acquisition size 5. Specifically, they observe that BALD several classes are under-represented in the acquisitions made by BALD. \cite{Christiano2017} use acquisition sizes of up to 500.

Finally, it is worth noting that sometimes random acquisition just does perform strongly. At the beginning of training, the uncertainty estimates used by the acquisition function may have biased noise, while random acquisition has no such bias. At the end of training, if most of the pool data have been acquired, active learning will show little improvement over random acquisition. Thus, depending on the setting, there is a potentially small window in which active learning may help. The usefulness of active learning depends on how large this window is. In active reward modelling, is it an open question how large this window is for different tasks.
%TODO is there any literature on the question of if/when rand acq just is very good? What do Yarin, Andreas, Houlsby graphs look like at beginning, end?
%TODO say something about it being an open question which acqisition function to use etc/active learning with deep NNs is still not super well understood?

\section{ARMA}
In this section we present the training protocol we developed. Specifically, we explain how to apply acquisition functions to reward modelling. %TODO do we really? what does this mean?
\begin{algorithm}
	\caption{ARMA: Active Reward Modelling for Agent Alignment.}
	\label{alg:arma}
	\begin{algorithmic}[1]
        \State Initialise RL agent
		\State Initialise neural network $ \rp $ as reward model
		\State Initialise experience buffer $ \expbuff $ for sampling clip pairs
		\State Initialise annotation buffer $ \annbuff $ for storing labelled clip pairs
		\State Define acquisition function $ a((\sigma^1, \sigma^2), \rp) $
		\State For each round $ i=1, \dots, N $ fix some number $ m_i $ of labels to request from the annotator in that round
		\State Without updating its parameters, run the agent in the environment and add experience to $ \expbuff $
		\For{$ i=1, \dots, N $}
		\State Sample $ 10m_i $ clip pairs from $ \expbuff $
		\State Acquire the $ m_i $ clip pairs that maximise $ a(.,.) $
		\State Request labels on these clip pairs (from the annotator) and add them to $ \annbuff $
		\State Reinitialise reward model $ \rp $
		\State Train $ \rp $ to convergence with the preferences in $ \annbuff $, by doing gradient descent on loss function \ref{eq:loss}
		\State Reinitialise RL agent
		\State Clear experience buffer $ \expbuff $
		\State Train RL agent to convergence with rewards from $ \rp $, adding experience to $ \expbuff $ \label{line:call_dqn}
		\EndFor
	\end{algorithmic}
\end{algorithm}
%TODO mention reinitialising agent (every round)?
%TODO mention reinitialsing reward model (before every time it is trained)? (cite Andreas/Yarin) Mention (capping) acquisition batch size?
%TODO in any case I don't want to confuse the algorithm so I'd probs mention these things in text below
%This follows \cite{Christiano2017}, which reduces the chance of the parameters of $ \rp $ collapsing to some bad local minimum before training begins. %TODO this is not a valid reason given that we reintialise the reward model before each training. So (as I have done) perhaps we can safely remove all the pretraining (at least from the simplified presentation of the algorithm). Though this would mean that the first round of agent training is pure rubbish (although, not in Acrobot :P -- on the contrary, that would have been v useful information!). Ok so now I've changed the order of the 1 1/2 loop to simplify the algorithm. I think this is more clear given our method. The old algorithm is in clippings.tex
In our implementation, we use DQN for our RL agent. Thus line \ref{line:call_dqn} represents calling Algorithm \ref{alg:dqn} as a subroutine, except with rewards from $ \rp $ instead of from the environment.

\section{Acquisition Functions and Uncertainty Estimates}
We tried each of the four acquisition functions explained in Section \ref{sec:acq_funcs}. Each require the ability to get uncertainty estimates by sampling from some appropriate posterior to the reward model $ \rp $ that we are optimizing. For this, we parameterise $ \rp $ with an ensemble of 5 neural networks. Each network has a different random initialisation and is trained independently, that is to say, using independent random minibatches for gradient descent\footnote{TODO mention that there is no principled way to implement ensemble, and that using different minibatches is a design choice?}. For a given input $ (s,a) \in \mathcal{S} \times \mathcal{A} $, we can draw 5 samples from the approximate posterior of $ \rp $: one for each forward pass through a network in the ensemble. There are other methods for sampling from an approximate posterior (see Section \ref{sec:bnns}), but we choose this method because while it is computationally more expensive than, for example, MC-Dropout, it introduces no additional hyperparameters to be tuned. This minimises the number of possible failure modes of our implementation, facilitating easier diagnosis of unsuccessful experiments.
%TODO what is the source to justify the use of ensemble as approximate posterior?

\section{Implementation Details}
The training protocol is implemented mostly in Python \cite{van1995python}. We use gradient descent to optimize the parameters of the deep Q-network and reward model. Pytorch \cite{paszke2017automatic} is an open source machine learning library, built on top of Python, which provides tools to perform automatic differentiation. This allows us to compute gradient without differentiating by hand our loss function with respect to our model parameters. We implement the buffers for collecting agent experience, storing annotated clips in SciPy \cite{jones2001}, which is also built on top of Python. This gives finer control over data representation and sampling.

\chapter{Experimental Details}
%TODO does it make sense to separate experiment details from method? I think so.
% Here I'll give the experimental details
% Gym. incl. why I chose it
% Cartpole/the envs I use
% Hyperparameter settings (and all the other args e.g. number of labels acquired per round, number of repetitions etc.) (maybe put these in appendix)
% Links to code
We used the environments provided by OpenAI Gym to run our experiments.

\section{CartPole-v0}

\chapter{Results}
%TODO does it make sense to separate experiment details from results? I'm less certain.
% Here I'll describe the results in a shiny way
% Graphs and comments on whether I achieved the goal of applying active learning to increase the sample efficiency of reward modelling
\section{CartPole-v0}
%\includegraphics[scale=1]{./results_A1}

\chapter{Conclusions}
\section{Summary}
\section{Evaluation}
% critical assessment of the work that has been done and the process of doing it
% perhaps include subsections for approaches that were tried and did not work; and personal development
\section{Future Work}

% acknowledgements

\bibliographystyle{apalike}
\bibliography{library,additional}

\appendix
\appendixpage
\noappendicestocpagenum
\addappheadtotoc
\chapter{Some Appendix Material}

\end{document}