\documentclass[11pt, a4paper, bibliography=totoc]{report}
\usepackage{amsmath,amsthm,amssymb, tabularx, listings, enumerate, cancel, bussproofs, tabto, wasysym, algpseudocode, algorithm, savesym, mathtools, physics, xcolor, setspace, appendix, dirtree}
\usepackage[margin=3.5cm]{geometry}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage[nottoc]{tocbibind} % adds Bibliography to 
\usepackage[linktocpage]{hyperref}

\hypersetup{
	colorlinks,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}
\doublespacing
\pagestyle{headings}

\newcommand{\nats}{\mathbb{N}}
\newcommand{\reals}{\mathbb{R}}
\renewcommand{\P}[1]{\mathbb{P}\left( #1 \right) }
%\newcommand{\E}[1]{\mathbb{E} \left[ #1 \right] }
\newcommand{\E}[2]{\mathbb{E}_{#1} \left[ #2 \right] }
\newcommand{\V}[2]{\mathbb{V}ar_{#1} \left[ #2 \right]}
\newcommand{\KLD}[2]{\mathrm{KL} \left[ \left. \left. #1 \right|\right| #2 \right] }

\newcommand{\w}{\mathbf{w}}
\newcommand{\data}{\mathcal{D}}
\newcommand{\vfe}{\mathcal{F}(\data, \theta)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\bepsilon}{\pmb{\epsilon}}
\newcommand{\bmu}{\pmb{\mu}}
\newcommand{\bsigma}{\pmb{\sigma}}
\newcommand{\btheta}{\pmb{\theta}}
\newcommand{\brho}{\pmb{\rho}}
\newcommand{\normal}[3]{\mathcal{N}(#1 \vert #2, #3)}
\newtheorem{claim}{Claim}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

\begin{document}
\title{Active Learning for Reward Modelling}
\author{Sam Clarke}
\date{September 2019}
\renewcommand{\bibname}{References}
\maketitle

\begin{abstract} % ~200 words

\end{abstract}

\tableofcontents
% say somewhere around here about collaboration with Zac and Angelos
% and where do I say who my supervisor is?

\chapter{Introduction}

\section{Relation to Material Studied on the MSc Course}

\chapter{Reinforcement Learning} % Explain problem, give context
Reinforcement learning (RL) refers simultaneously to a problem, methods for solving that problem, and the field that studies the problem and its solution methods. The problem of RL is to learn what to do---how to map situations to actions---so as to maximise some numerical reward signal \cite[pp.~1-2]{Sutton2018}.

In this section we first introduce the elements of RL informally. We then formalise the RL Problem as the optimal control of incompletely-known Markov decision processes (finite? do I talk about PO?). We give a taxonomy of different RL solution methods and conclude with a description of one such method, Deep Q-Learning (DQN) that is of particular importance in this dissertation.

\section{Elements of Reinforcement Learning}
This subsection will introduce agent, environment, policy, reward signal, value function and [model] informally, similar to S\&B 1.3. Is this necessary or should I skip straight to the formalism?

\section{Finite Markov Decision Processes}
Finite Markov Decision Processes (finite MDPs) are a way of mathematically formalising the RL problem: they capture the most important aspects of the problem faced by an agent interacting with its environment to achieve a goal. We introduce the elements of this formalism: the agent-environment interface, goals and rewards, returns and episodes. Then...

\subsection{The Agent-Environment Interface}
MDPs consist firstly of the continual interaction between an agent selecting actions, and an environment responding by changing state, and presenting the new state to the agent, along with an associated scalar reward. Recall that the agent seeks to maximise this reward over time through its choice of actions.

More formally, consider a sequence of discrete time steps, $t = 1,2,3, \dots$. At each time step $t$, the agent receives some representation of the environment's \textit{state}, $ S_t \in \mathcal{S} $, and chooses an \textit{action}, $ A_t \in \mathcal{A} $. On the next time step, the agent receives reward $ R_{t+1} \in \mathcal{R} \subset \reals $, and finds itself in a new state, $ S_{t+1} $. These interactions repeat over time, giving rise to a \textit{trajectory}:
\begin{align*}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \dots
\end{align*}
% TODO insert Figure 3.1-like figure from S&B
% TODO ask best practices for figures
% TODO ask best practices for latex commands

A \textit{finite} MDP is one where the sets of states, actions and rewards are finite. In this case, the random variables $ S_t $ and $ R_t $ have well-defined discrete probability distributions which depend only on the preceding state and action. This allows us to define the \textit{dynamics} of the MDP, a probability mass function $ p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \mapsto [0,1] $, as follows. For any particular values $ s' \in \mathcal{S} $ and $ r \in \mathcal{R} $ of the random variables $ S_t $ and $ R_t $, there is a probability of these values occurring at time $ t $, given any values of the previous state $ s \in \mathcal{S} $ and action $ a \in \mathcal{A} $:
\begin{align*}
p(s', r \mid s, a) := \P{S_t = s', R_t = r \mid S_{t-1} = s , A_{t-1} = a }.
\end{align*}

A \textit{Markov} Decision Process is one where all states satisfy the Markov property. A state $ s_t $ of an MDP satisfies this property iff:
\begin{align*}
\P{s_{t+1}, r_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \dots, s_0, a_0 } = \P{s_{t+1} \mid s_t, a_t}.
\end{align*}
This implies that the immediately preceding state $ s_t $ and action $ a_t $ are sufficient statistics for predicting the next state $ s_{t+1} $ and reward $ r_{t+1} $.

\subsection{Goals and Rewards}
The reader may have noticed that we first introduced MDPs as a formalism for an agent interacting with its environment to achieve a goal, yet have since spoken instead of maximising a reward signal $ R_t \in \reals $ over time. Our implicit assumption is the following hypothesis:
\begin{hypothesis}[Reward Hypothesis]
\textit{All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).} \cite[p.~53]{Sutton2018}
\end{hypothesis}
However, this hypothesis gives no information about how to construct such a scalar signal; only that it exists. Indeed, recent work has shown that it is far from trivial to do so; possible failure modes include negative side effects, reward hacking and unsafe exploration \cite{Amodei2016}. This is central to the topic of this dissertation---our aim is to improve the sample efficiency of one particular method of reinforcement learning when the reward signal is unknown.

\subsection{Returns and Episodes} \label{rets_and_episodes}
Having asserted that we can express the objective of reinforcement learning in terms of scalar reward, we now formally define this objective. Consider the following objective:
\begin{definition}[(Future discounted) return]
Let a sequence of rewards between time step $ t + 1 $ and $ T $ (inclusive) be $ R_{t+1}, R_{t+1}, \dots, R_T $. Let $ \gamma \in [0, 1] $ be a discount factor of future rewards. Then we define the (future discounted) return of this sequence of rewards \cite[p.~57]{Sutton2018}:
\begin{equation} \label{G_t}
G_t := \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}.
\end{equation}
\end{definition}

One reason for introducing a discount factor is because we would like this infinite sum to converge. Accordingly, we impose the condition that $ \gamma < 1 $ whenever the reinforcement learning task is \textit{continuous}, that is to say, there may be an infinite number of non-zero terms in the sequence of rewards $ \{R_{t+1}, R_{t+2}, R_{t+3}, \dots \} $.

The other kind of task is called \textit{episodic}. Here, interactions between the agent and environment occur in well-defined subsequences, each of which ends in a special \textit{terminal state}. The environment then resets to a starting state, which may be fixed or sampled from a distribution. To adapt the definition in (\ref{G_t}) to this case, we introduce the convention that zero reward is given after reaching the terminal state. This is because we typically analyse such tasks by considering a single episode---either because we care about that episode in particular, or something that holds across all episodes \cite[p.~57]{Sutton2018}. Observe that summing to infinity in (\ref{G_t}) is then identical to summing over the episode, and that the sum is well-defined regardless of the discount factor $ \gamma $.

\subsection{Policies and Value Functions} \label{policy_value_functions}
\textit{Policy} determines the behaviour of the agent. Formally, a policy $ \pi : \mathcal{S} \times \mathcal{A} \mapsto [0,1] $ defines a probability distribution over actions, given a state. That is to say, $ \pi(a \mid s) $ is the probability of selecting action $ a $ if an agent is following policy $ \pi $ and in state $ s $.

The \textit{state-value function} $ v_\pi : \mathcal{S} \mapsto \reals $ for a policy $ \pi $ gives the expected return of starting in a state and following that policy. More formally,
\begin{definition}[State-value function]
	Let $ \pi $ be a policy and $ s \in \mathcal{S} $ be any state. We write $ \E{\pi}{.}$ to denote the expected value of the random variable $ G_t $ as defined in (\ref{G_t}). Then the state-value function (or simply, value function) for policy $ \pi $ is:
	\begin{equation} \label{v_pi}
	v_\pi(s) := \E{\pi}{G_t \mid S_t = s} = \E{\pi}{\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s}.
	\end{equation}
\end{definition}

The \textit{action-value function} $ q_\pi : \mathcal{S} \times \mathcal{A} \mapsto \reals $ for a policy $ \pi $ is defined similarly. It gives the expected return of starting in a state, taking a given action, and following policy $ \pi $ thereafter.
\begin{definition}[Action-value function]
	Let $ \pi $ be a policy, $ s \in \mathcal{S} $ be any state and $ a \in \mathcal{A} $ any action. Then the action-value function (or, Q-function) for policy $ \pi $ is:
	\begin{equation} \label{v_pi}
	q_\pi(s, a) := \E{\pi}{G_t \mid S_t = s, A_t = a} = \E{\pi}{\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s, A_t = a}.
	\end{equation}
\end{definition}

\subsection{Optimal Policies and Optimal Value Functions} \label{optimal_policy_value_functions}
%TODO I'm confused about optimal policy. On Spinningup it seems to be defined as the policy that maximises the v_\pi(s_0). But S&B say it's the policy that maximises v_\pi(s) for all s \in S. These two defs are inconsistent bc consider an unreachable state? Spinningup def could say that some policy is optimal despite being bad in unreachable state; S&B def would not...
% http://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-rl-problem
% p.64 S&B
The problem of Reinforcement Learning is thus to find an optimal policy.

All optimal policies share the same value functions. We call these the \textit{optimal state-value function}, $ v_* $, and the \textit{optimal action-value function} $ q_* $:
\begin{definition}[Optimal state-value function (from {\cite[p.~62]{Sutton2018}})]
	$$ v_*  := \max_\pi v_\pi(s) ~~ \forall s \in \mathcal{S} .$$
\end{definition}
\begin{definition}[Optimal action-value function (from {\cite[p.~63]{Sutton2018}})]
	$$ q_*  := \max_\pi q_\pi(s,a) ~~ \forall s \in \mathcal{S} ~~ \forall a \in \mathcal{A} .$$
\end{definition}

There is a simple connection between optimal Q-function and optimal policy that will be used in Section \ref{RL_solution_methods}:
\begin{claim} \label{Q_claim}
	If an agent has $ q_* $, then acting according to the optimal policy when in some state $ s $ is as simple as finding the action $ a $ that maximises $ q_*(s,a) $ \cite[p.~64]{Sutton2018}.
\end{claim}

\subsection{Bellman Equations}
These value functions obey special recursive relationships called Bellman equations. The equations are proved by formalising the simple idea that the value of being in a state is the expected reward of that state, plus the value of the next state you move to. Each of the four value functions defined in Sections \ref{policy_value_functions} and \ref{optimal_policy_value_functions} satisfy slightly different equations. We prove the Bellman equation for the value function and state the remaining three for completeness.

\begin{proposition}[Bellman equation for $ v_\pi $ {\cite[p.~59]{Sutton2018}}]
	Let $ \pi $ be a policy, $ p $ the dynamics of an MDP, $ \gamma $ a discount factor and $ v_\pi $ a state-value function. Then:
	\begin{align}
		v_\pi(s) = \underset{\substack{a \sim \pi(. \mid s) \\ s', r \sim p(., . \mid s, a) }}{\mathbb{E}} \left[ r + \gamma v_\pi(s') \right]
	\end{align}
\end{proposition}
\begin{proof}
	\begin{align*}
		v_\pi(s) &:= \E{\pi}{G_t \mid S_t = s} \\
		         &= \E{\pi}{\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s} \\
		         &= \E{\pi}{R_{t+1} + \gamma G_{t+1}} \\
		         &= \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{\substack{s' \in \mathcal{S} \\ r \in \mathcal{R}}} p(s', r \mid s, a) \left[ r + \gamma \E{\pi}{G_{t+1} \mid S_{t+1} = s'} \right] \\
		         &= \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{\substack{s' \in \mathcal{S} \\ r \in \mathcal{R}}} p(s', r \mid s, a) \left[ r + \gamma  v_\pi(s') \right] \\
		         &= \underset{\substack{a \sim \pi(. \mid s) \\ s', r \sim p(., . \mid s, a) }}{\mathbb{E}} \left[ r + \gamma v_\pi(s') \right]
	\end{align*}
\end{proof}

\begin{proposition}[Bellman equation for $ q_\pi $]
	Let $ \pi $ be a policy, $ p $ the dynamics of an MDP, $ \gamma $ a discount factor and $ q_\pi $ an action-value function. Then:
	\begin{align}
	q_\pi(s, a) = \underset{s', r \sim p(., . \mid s, a)}{\mathbb{E}} \left[ r + \gamma \underset{a' \sim \pi(.\mid s')}{\mathbb{E}}\left[q_\pi(s', a')\right] \right]
	\end{align}
\end{proposition}

\begin{proposition}[Bellman equation for $ v_* $ {\cite[p.~63]{Sutton2018}}]
	Let $ p $ be the dynamics of an MDP, $ \gamma $ a discount factor and $ v_* $ an optimal value function. Then:
	\begin{align}
	v_*(s) = \max_{a \in \mathcal{A}} ~ \underset{s', r \sim p(., . \mid s, a)}{\mathbb{E}} \left[ r + \gamma v_*(s') \right]
	\end{align}
\end{proposition}

\begin{proposition}[Bellman equation for $ q_* $ {\cite[p.~63]{Sutton2018}}] \label{bellman_q*}
	Let $ p $ be the dynamics of an MDP, $ \gamma $ a discount factor and $ q_* $ an optimal Q-function. Then:
	\begin{align}
	q_*(s, a) = \underset{s', r \sim p(., . \mid s, a)}{\mathbb{E}} \left[ r + \gamma ~ \underset{a' \in \mathcal{A}}{\max}\left[q_*(s', a')\right] \right]
	\end{align}
\end{proposition}

\section{Reinforcement Learning Solution Methods} \label{RL_solution_methods}
One method of solving the reinforcement learning problem is to explicitly solve a set of Bellman optimality equations. For example, in a finite MDP with $ n $ states and $ m $ actions, the Bellman equations for $ q_* $ are a set of $ n\cdot m $ equations in $ n\cdot m $ unknowns\footnote{This assumes that the agent can take any action in any state.}. Given the dynamics $ p $ of the MDP, standard techniques for solving systems of equations can be applied. Then, via Claim (\ref{Q_claim}), the agent has an optimal policy \cite[p.~64]{Sutton2018}.

However, in reality, we rarely have access to $ p $, or sufficient computational resources to solve this system of equations exactly \cite[p.~66]{Sutton2018}. Thus, the literature on RL solution methods focuses on finding approximate solutions.

In this section, we give a brief and incomplete taxonomy of RL solution methods, most of which give approximate solutions. Then, we focus on one such method, the deep Q-network, which is important for the rest of this thesis.

\subsection{Taxonomy of RL Solution Methods}
The following taxonomy draws on those given in \cite{Sutton2018} and \cite{Achiam2019}.

\dirtree{%
	.1 RL Solution Methods.
	.2 Tabular Solution Methods.
	.3 Dynamic Programming.
	.4 Policy Iteration.
	.4 Value Iteration.
	.3 Monte Carlo Methods.
	.4 MC Prediction.
	.4 MC Control.
	.3 Temporal-Difference Learning.
	.4 Sarsa.
	.4 Q-Learning.
	.2 Approximate Solution Methods.
	.3 Deep RL.
	.4 Model-Free.
	.5 Policy Optimisation.
	.6 Policy Gradient.
	.6 A2C / A3C.
	.6 PPO.
	.6 TPRO.
	.5 Q-Learning.
	.6 \textbf{DQN}.
	.6 C51.
	.5 Policy Optimisation + Q-Learning.
	.6 DDPG.
	.6 TD3.
	.6 SAC.
	.4 Model-Based.	
	.5 Given the Model.
	.6 AlphaZero.
	.5 Learn the Model.
	.6 I2A.
}
% design choices; trade-offs?
% model-based v model-free

\subsection{Deep Q-network} \label{DQN}
The deep Q-network (DQN) agent approximates $ q_* $ using a deep convolutional neural network $ Q(s,a; \theta) $ as the function approximator \cite{Mnih2015}. $ \theta $ are the parameters (weights) of the neural network, which is called a Q-network. It can be trained by performing gradient descent on the parameters $ \theta_i $ at iteration $ i $ to reduce the mean-squared error between $ Q(s,a; \theta_i) $ and the Bellman equation for $ q_*(s,a) $, given in Proposition (\ref{bellman_q*}). Since we do not have access to the true value of this target, we instead use approximate target values $ y = r + max_{a'} ~ Q(s', a' ; \theta_i^-) $, with $ \theta_i^- $ some previous network parameters.

One could then perform standard gradient descent on this loss function using the experience collected by the agent $\langle (s_t, a_t, r_{t+1}, s_{t+1}) \rangle_{t=0}^T $ as data, as one would do in the supervised learning setting. However, there are three important differences in the reinforcement learning setting: correlations both (i) in the data set, and (ii) between $ Q(s,a ; \theta_i) $ and the targets. Furthermore, (iii) updates to $ Q $ may change the policy and thus change the data distribution. This leads to instability in training. To address this, the authors propose two algorithmic innovations. Firstly, instead of training on experience in the order that it is collected, the agent maintains a buffer of experience $ D_t = \{ e_1, e_2, \dots, e_t \} $ where $ e_t = (s_t, a_t, r_{t+1}, s_{t+1}) $. When making learning updates, drawing minibatches uniformly at random from this buffer breaks correlations in the experience sequence and smooths over changes in the data distribution, alleviating problems (i) and (iii). This is termed \textit{experience replay}. Secondly, to reduce correlations between $ Q $ and the targets and alleviate problem (ii), the approximate target values are updated to match the parameters $ Q $ only every $ C $ steps for some hyperparameter $ C > 1 $.

With these changes, we arrive at a loss function $ \ell_i(\theta_i) $ for each learning update $ i $:
\begin{align*}
\ell_i(\theta_i) &= \E{ s,a,r }{ (\E{s'}{y \mid s,a} - Q(s,a;\theta_i))^2 } \\
                 &= \E{ s,a,r }{ \E{s'}{y - Q(s,a;\theta_i) \mid s,a } ^2 } \\
                 &= \E{ s,a,r }{ \E{s'}{(y - Q(s,a;\theta_i))^2 \mid s,a}  - \V{s'}{y - Q(s,a;\theta_i)) \mid s,a}   } \\
                 &= \E{s,a,r,s'}{(y - Q(s,a;\theta_i))^2} - \E{s,a,r}{\V{s'}{y}}
\end{align*}
where the expectations and variances are with respect to samples from the experience replay. This loss function is then optimized by stochastic gradient descent with respect to the network parameters $ \theta_i $. Note that the final term is independent of these parameters, so we can ignore it. Finally, the authors found that stability is improved by clipping the error term $ y - Q(s,a;\theta_i) $ to be between $ -1 $ and $ 1 $.

We summarise this training procedure in Algorithm \ref{alg:dqn}. To ensure adequate exploration, the agent's policy is $ \epsilon $-greedy with respect to the current estimate of the optimal action-value function.

\begin{algorithm}
	\caption{Deep Q-learning with experience replay.}
	\label{alg:dqn}
	\begin{algorithmic}[1]
		\State Initialise replay memory $ D $ to capacity $ N $
		\State Initialise neural network $ Q $ with random weights $ \theta $ as approximate optimal action-value function
		\State Initialise neural network $ \hat{Q} $ with identical weights $ \theta^- = \theta $ as approximate target action-value function
		\State Reset environment to starting state $ s_0 $
		\For{$ t=0, \dots, T $}
		\State With probability $ \epsilon $ execute random action $ a_t $
		\State otherwise execute action $ a_t = \arg\max_a Q(s_t, a ; \theta) $
		\State Observe next state and corresponding reward $ s_{t+1}, r_{t+1} \sim p(.,.\mid s_t,a_t) $
		\State Store transition $ (s_t, a_t, r_{t+1}, s_{t+1}) $ in $ D_t $
		\State Randomly sample minibatch of transitions $ (s_j, a_j, r_{j+1}, s_{j+1}) \sim D_t $
		\State Set $ y_j = \begin{cases}
		                   r_{j+1} \text{ if episode terminates at step $ j+1 $} \\
		                   r_{j+1} + \gamma \max_{a'} \hat{Q}(s_{j+1}, a_j ; \theta^- ) \text{ otherwise}
		\end{cases}$
		\State Do gradient descent on $ (y_j - Q(s_j, a_j ; \theta))^2 $ w.r.t network parameters $ \theta $
		\State Every $ C $ steps set $ \theta^- = \theta $
		\EndFor
	\end{algorithmic}
\end{algorithm}

Note that line 11 assumes we are training DQN to perform an episodic task, hence the first case which follows the convention given in Section \ref{rets_and_episodes} whereby zero reward is given for all states after the terminal state. If the task were instead continuing, line 11 would simply be $ y_j = r_{j+1} + \gamma \max_{a'} \hat{Q}(s_{j+1}, a_j ; \theta^- ) $.

%TODO do I need to describe neural networks too?

\section{Reinforcement Learning from Unknown Reward Functions}
For many domains in which we might want to use RL, states of the environment are not inherently associated with rewards.  Thus, we have the additional task of specifying a reward function, which maps states to rewards. Argue the case that directly specifying a reward function is hard, in order to motivate the next section: Reward Learning
% We can use feedback of different modalities
% There is both the standard and the deep case to consider
\subsection{Reward Learning with Handcrafted Feature Transformations}

\subsubsection{Active Preference-Based Learning of Reward Functions}

\subsubsection{Batch Active Preference-Based Learning of Reward Functions}

\subsubsection{DemPref}
% Summarise the contributions of the 2017,18,19 papers by Sadigh's lab.
% (+Batch) Active Preference-Based Learning of Reward Functions (ignore the active part here)
% DemPref Learning Reward Functions by Integrating Human Demonstrations and Preferences
\subsection{Reward Learning in Deep RL}
% Christiano; Ibarz. Basically use the Context section of Ibarz -- this will make it easy!
% perhaps mention SARM Agenda

\chapter{Uncertainty in Deep Learning}
% Talk about Yarin's thesis, BNNs; maybe Ian Obsband paper for good measure (esp. if I end up using ensemble instead of MC-Dropout?)
Standard deep learning models output point estimates. For example, a model trained to classify pictures of dogs according to their breed takes a picture of a dog and outputs its predicted breed. However, what will the model do if it is given a picture of a cat? \cite{Gal2017a}.

We probably want the model to be able to recognise that this is an out of distribution example, and request more training data, or simply say that it doesn't know the answer. However, since standard deep learning models output only point estimates, the model will just go ahead and classify the cat as some breed of dog, just as confidently as any other input.

Thus, the field of Bayesian Deep Learning aims to equip neural networks with the ability to output a point estimate along with its uncertainty in that estimate. Historically, many of the attempts to do so were not very practical. For example, one algorithm, Bayes by Backprop, requires doubling the number of model parameters, making training more computationally expensive, and is very sensitive to hyperparameter tuning. However, recent techniques allow almost any network trained with a stochastic regularisation technique, such as dropout, to, given an input, obtain a predictive mean and variance (uncertainty), without any complicated augmentation to the network \cite[p.~15]{Gal2017a}.

\section{Bayesian Neural Networks}

\section{Model Uncertainty in BNNs}

\chapter{Active Learning}
% BALD (Houlsby: Bayesian Active Learning for Classification and Preference Learning)
% Adapting it to the deep case (Yarin's thesis/Image Data paper)
\section{Acquisition Functions}

\subsection{Max Entropy}

\subsection{Variation Ratios}

\subsection{Mean STD}

\subsection{BALD}

\section{Applying Active Learning to RL without a reward function}
Discussion of previous work.

\subsection{APRIL}
% APRIL

\subsection{Active Preference-Based Learning of Reward Functions with handcrafted feature transformations}
% Summarise the *active learning parts* of the 2017,18,19 papers by Sadigh's lab (I've already summarised the 
% reward learning parts, above)
% it probably doesn't make sense to have a subsubsection for each, as I assume the active parts are basically the same?
% but work this out once you've worked out what the differences are

\subsubsection{Active Preference-Based Learning of Reward Functions}

\subsubsection{Batch Active Preference-Based Learning of Reward Functions}

\subsubsection{DemPref}
% Learning Reward Functions by Integrating Human Demonstrations and Preferences (DemPref) (they use the `Volume Removal Method' to do Active Learning)

\subsection{Deep RL from Human Preferences}
% Christiano: summarise what didn't work, and what might have gone wrong (how to frame this depends on the results that I eventually get to)

\chapter{Method}
% Here I'll describe the training protocol I used (mostly Ibarz but without the demos)
% I can leave gorey detail to Experiments/Appendix
% I should have already summarised the high level approach in Background
% And I'll describe the different methods of Active Learning/uncert estimates that I tried
% Implementational matters: PyTorch and why I chose it
\section{Training Protocol}

\section{Acquisition Functions}

\section{Uncertainty Estimates}

\section{Implementation Details}

\chapter{Experimental Details}
%TODO does it make sense to separate experiment details from method? I think so.
% Here I'll give the experimental details
% Gym. incl. why I chose it
% Cartpole/the envs I use
% Hyperparameter settings (and all the other args e.g. number of labels acquired per round, number of repetitions etc.) (maybe put these in appendix)
% Links to code
We used the environments provided by OpenAI Gym to run our experiments.

\section{CartPole-v0}

\chapter{Results}
%TODO does it make sense to separate experiment details from results? I'm less certain.
% Here I'll describe the results in a shiny way
% Graphs and comments on whether I achieved the goal of applying active learning to increase the sample efficiency of reward modelling
\section{CartPole-v0}


\chapter{Conclusions}
\section{Summary}
\section{Evaluation}
% critical assessment of the work that has been done and the process of doing it
% perhaps include subsections for approaches that were tried and did not work; and personal development
\section{Future Work}

% acknowledgements

\bibliographystyle{plain}
\bibliography{MSc}

\appendix
\appendixpage
\noappendicestocpagenum
\addappheadtotoc
\chapter{Some Appendix Material}

\end{document}