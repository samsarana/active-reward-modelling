\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Background}{4}{part.1}\protected@file@percent }
\citation{silver2016mastering}
\citation{alphastarblog}
\citation{Leike2018}
\citation{Ng2000}
\citation{Ziebart2008}
\citation{Christiano2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{bellemare2013arcade}
\citation{todorov2012mujoco}
\citation{Christiano2017}
\citation{Ibarz2018}
\citation{Christiano2017}
\citation{Christiano2017}
\citation{henderson2018deep}
\citation{Christiano2017}
\citation{Sutton2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reinforcement Learning}{8}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Elements of Reinforcement Learning}{8}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Finite Markov Decision Processes}{8}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Agent-Environment Interface}{9}{subsection.2.2.1}\protected@file@percent }
\newlabel{subsection:agent_env_interface}{{2.2.1}{9}{The Agent-Environment Interface}{subsection.2.2.1}{}}
\citation{Sutton2018}
\citation{Amodei2016}
\citation{Sutton2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Goals and Rewards}{10}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Returns and Episodes}{10}{subsection.2.2.3}\protected@file@percent }
\newlabel{rets_and_episodes}{{2.2.3}{10}{Returns and Episodes}{subsection.2.2.3}{}}
\citation{Sutton2018}
\newlabel{G_t}{{2.1}{11}{(Future discounted) return}{equation.2.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Policies and Value Functions}{11}{subsection.2.2.4}\protected@file@percent }
\newlabel{policy_value_functions}{{2.2.4}{11}{Policies and Value Functions}{subsection.2.2.4}{}}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{Sutton2018}
\newlabel{v_pi}{{2.2}{12}{State-value function}{equation.2.2.2}{}}
\newlabel{q_pi}{{2.3}{12}{Action-value function}{equation.2.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Optimal Policies and Optimal Value Functions}{12}{subsection.2.2.5}\protected@file@percent }
\newlabel{optimal_policy_value_functions}{{2.2.5}{12}{Optimal Policies and Optimal Value Functions}{subsection.2.2.5}{}}
\newlabel{Q_claim}{{1}{12}{}{claim.1}{}}
\citation{Sutton2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Bellman Equations}{13}{subsection.2.2.6}\protected@file@percent }
\citation{Sutton2018}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{lillicrap2015continuous}
\citation{fujimoto2018addressing}
\citation{haarnoja2018soft}
\newlabel{bellman_q*}{{4}{14}{Bellman equation for $ q_* $ {\cite [p.~63]{Sutton2018}}}{proposition.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning Solution Methods}{14}{section.2.3}\protected@file@percent }
\newlabel{RL_solution_methods}{{2.3}{14}{Reinforcement Learning Solution Methods}{section.2.3}{}}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Deep Neural Networks}{15}{subsection.2.3.1}\protected@file@percent }
\newlabel{sec:dnn}{{2.3.1}{15}{Deep Neural Networks}{subsection.2.3.1}{}}
\newlabel{eg:nn}{{1}{15}{}{example.1}{}}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {subsubsection}{Cost function}{16}{section*.2}\protected@file@percent }
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\newlabel{eq:nll}{{2.8}{17}{Cost function}{equation.2.3.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Optimization procedure}{17}{section*.3}\protected@file@percent }
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\citation{tieleman2012lecture}
\citation{Goodfellow-et-al-2016}
\citation{Mnih2015}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces RMSProp.\relax }}{19}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:rmsprop}{{1}{19}{RMSProp.\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Deep Q-network}{19}{subsection.2.3.2}\protected@file@percent }
\newlabel{DQN}{{2.3.2}{19}{Deep Q-network}{subsection.2.3.2}{}}
\citation{Christiano2017}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Deep Q-learning with experience replay.\relax }}{21}{algorithm.2}\protected@file@percent }
\newlabel{alg:dqn}{{2}{21}{Deep Q-learning with experience replay.\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Reinforcement Learning from Unknown Reward Functions}{21}{section.2.4}\protected@file@percent }
\citation{Ng2000}
\citation{Ziebart2008}
\citation{Ho2016}
\citation{Hester2017}
\citation{Knox2009}
\citation{Warnell2017}
\citation{Wilson2012}
\citation{Christiano2017}
\citation{Ibarz2018}
\gdef \LT@i {\LT@entry 
    {1}{99.88464pt}\LT@entry 
    {1}{99.48465pt}\LT@entry 
    {1}{99.48465pt}\LT@entry 
    {1}{99.48465pt}}
\citation{Ibarz2018}
\citation{bellemare2013arcade}
\citation{Ibarz2018}
\citation{Palan2019}
\citation{Byk2017}
\citation{brockman2016gym}
\citation{Palan2019}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Summary of the properties of using different forms of human feedback in RL without a reward function.\relax }}{23}{table.2.1}\protected@file@percent }
\newlabel{table:1}{{2.1}{23}{Summary of the properties of using different forms of human feedback in RL without a reward function.\relax }{table.2.1}{}}
\newlabel{footnote:exploration}{{6}{23}{}{Hfootnote.5}{}}
\citation{Byk2017}
\citation{Christiano2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Reward Learning from Trajectory Preferences in Deep RL}{24}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Setting}{24}{section*.4}\protected@file@percent }
\citation{Christiano2017}
\citation{Ho2016}
\citation{Mnih2016}
\citation{Schulman2015}
\citation{Ibarz2018}
\citation{Christiano2017}
\citation{Ibarz2018}
\citation{Hester2017}
\@writefile{toc}{\contentsline {subsubsection}{Method}{25}{section*.5}\protected@file@percent }
\citation{Ibarz2018}
\citation{Christiano2017}
\@writefile{toc}{\contentsline {subsubsection}{Process 1: Training the policy}{26}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Process 2: Selecting and annotating clip pairs}{26}{section*.7}\protected@file@percent }
\citation{elo1978rating}
\citation{Byk2017}
\@writefile{toc}{\contentsline {subsubsection}{Process 3: Training the reward model}{27}{section*.8}\protected@file@percent }
\newlabel{assumption:1}{{1}{27}{}{assumption.1}{}}
\newlabel{eq:softmax}{{2.10}{27}{Process 3: Training the reward model}{equation.2.4.10}{}}
\newlabel{eq:loss}{{2.11}{27}{Process 3: Training the reward model}{equation.2.4.11}{}}
\citation{Christiano2017}
\citation{Christiano2017}
\citation{Byk2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Reward Learning from Trajectory Preferences with Handcrafted Feature Transformations}{28}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Setting}{28}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Method}{28}{section*.10}\protected@file@percent }
\citation{Christiano2017}
\citation{Gal2017a}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Uncertainty in Deep Learning}{30}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Blundell2015}
\citation{Gal2017a}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Bayesian Neural Networks}{31}{section.3.1}\protected@file@percent }
\citation{cohn1996active}
\citation{Gal2017b}
\citation{Gal2017a}
\citation{shannon1948mathematical}
\citation{Gal2017a}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Active Learning}{32}{section.3.2}\protected@file@percent }
\citation{Gal2017a}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Max Entropy}{33}{subsection.3.2.1}\protected@file@percent }
\newlabel{sec:max_ent}{{3.2.1}{33}{Max Entropy}{subsection.3.2.1}{}}
\newlabel{eq:entropy}{{3.1}{33}{Max Entropy}{equation.3.2.1}{}}
\newlabel{ex:max_ent}{{2}{33}{}{example.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}BALD}{33}{subsection.3.2.2}\protected@file@percent }
\citation{Gal2017a}
\citation{Houlsby2011}
\newlabel{eq:bald}{{3.2}{35}{BALD}{equation.3.2.2}{}}
\citation{freeman1965elementary}
\citation{kampffmeyer2016semantic}
\citation{kendall2015bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Variation Ratios}{36}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Mean STD}{36}{subsection.3.2.4}\protected@file@percent }
\citation{Christiano2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Applying Active Learning to RL without a reward function}{38}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}APRIL}{38}{subsection.4.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.2}Active Preference-Based Learning of Reward Functions with handcrafted feature transformations}{38}{subsection.4.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.3}Deep RL from Human Preferences}{38}{subsection.4.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Innovation}{39}{part.2}\protected@file@percent }
\citation{Christiano2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Method}{40}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Possible failure modes of active reward modelling}{40}{section.5.1}\protected@file@percent }
\newlabel{sec:failure}{{5.1}{40}{Possible failure modes of active reward modelling}{section.5.1}{}}
\citation{Christiano2017}
\citation{Houlsby2011}
\citation{Gal2015}
\citation{Blundell2015}
\citation{Christiano2017}
\citation{Kirsch2019a}
\citation{Christiano2017}
\citation{Gal2017b}
\citation{Kirsch2019a}
\citation{cohen2017emnist}
\citation{Christiano2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Failure modes of active learning in general}{41}{subsection.5.1.1}\protected@file@percent }
\citation{Christiano2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Failure modes of active learning in the reward modelling setting}{42}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Active Reward Modelling}{44}{section.5.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Active Reward Modelling.\relax }}{44}{algorithm.3}\protected@file@percent }
\newlabel{alg:arma}{{3}{44}{Active Reward Modelling.\relax }{algorithm.3}{}}
\newlabel{line:sample_exp}{{9}{44}{Active Reward Modelling.\relax }{algorithm.3}{}}
\newlabel{line:acquire}{{10}{44}{Active Reward Modelling.\relax }{algorithm.3}{}}
\newlabel{line:train_r}{{12}{44}{Active Reward Modelling.\relax }{algorithm.3}{}}
\newlabel{line:call_dqn}{{16}{44}{Active Reward Modelling.\relax }{algorithm.3}{}}
\citation{van1995python}
\citation{paszke2017automatic}
\citation{jones2001}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Acquisition Functions and Uncertainty Estimates}{45}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Implementation Details}{45}{section.5.4}\protected@file@percent }
\citation{barto1983neuronlik}
\citation{brockman2016gym}
\citation{gym2019cartpole}
\citation{gym2019cartpole}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Experiments and Results}{46}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The CartPole environment \cite  {gym2019cartpole}\relax }}{46}{figure.caption.11}\protected@file@percent }
\newlabel{fig:cartpole}{{6.1}{46}{The CartPole environment \cite {gym2019cartpole}\relax }{figure.caption.11}{}}
\newlabel{sec:arma}{{6}{46}{Experiments and Results}{figure.caption.11}{}}
\citation{Christiano2017}
\citation{Christiano2017}
\citation{Christiano2017}
\citation{rlblogpost}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Mean episode return of agent trained via reward modelling using random acquisition and BALD. The performance of standard RL, a random policy, and an agent trained on a randomly initialised reward model are also shown. Results are averaged over 40 runs except for the random reward model condition which is averaged over 20. Error bars show standard error. Note that the performance of the agents trained via reward modelling is worse than those in the following sections due to less extensive hyperparameter tuning.\relax }}{48}{figure.caption.12}\protected@file@percent }
\newlabel{fig:cartpole_curve}{{6.2}{48}{Mean episode return of agent trained via reward modelling using random acquisition and BALD. The performance of standard RL, a random policy, and an agent trained on a randomly initialised reward model are also shown. Results are averaged over 40 runs except for the random reward model condition which is averaged over 20. Error bars show standard error. Note that the performance of the agents trained via reward modelling is worse than those in the following sections due to less extensive hyperparameter tuning.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Hypothesis 3: Reward model retraining}{48}{section.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Number of labels required to solve CartPole by reward modelling with random acquisition and BALD, with and without retraining reward model after each acquisition. Results are averaged over 20 runs; error bars show standard error.\relax }}{49}{figure.caption.13}\protected@file@percent }
\newlabel{fig:retrain}{{6.3}{49}{Number of labels required to solve CartPole by reward modelling with random acquisition and BALD, with and without retraining reward model after each acquisition. Results are averaged over 20 runs; error bars show standard error.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Hypothesis 4: Acquisition size}{49}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Hypotheses 1 and 2: Uncertainty estimates and acquisition functions}{49}{section.6.3}\protected@file@percent }
\citation{Kirsch2019a}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Number of labels required to solve CartPole by reward modelling with BALD, using acquisition sizes 1 and 10. Results are averaged over 20 repeats for acquisition size 10 and 6 repeats for acquisition size 1; error bars show standard error.\relax }}{50}{figure.caption.14}\protected@file@percent }
\newlabel{fig:acq_size}{{6.4}{50}{Number of labels required to solve CartPole by reward modelling with BALD, using acquisition sizes 1 and 10. Results are averaged over 20 repeats for acquisition size 10 and 6 repeats for acquisition size 1; error bars show standard error.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Number of labels required to solve CartPole by reward modelling using various acquisition functions. The performance of using random acquisition with an ensemble of reward models, and BALD with uncertainty estimates from MC-Dropout with is also shown. Results are averaged over 20 repeats; error bars show standard error.\relax }}{51}{figure.caption.15}\protected@file@percent }
\newlabel{fig:cartpole_final}{{6.5}{51}{Number of labels required to solve CartPole by reward modelling using various acquisition functions. The performance of using random acquisition with an ensemble of reward models, and BALD with uncertainty estimates from MC-Dropout with is also shown. Results are averaged over 20 repeats; error bars show standard error.\relax }{figure.caption.15}{}}
\citation{Kirsch2019a}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Acquiring a batch of clip pairs with acquisition size 1.\relax }}{52}{algorithm.4}\protected@file@percent }
\newlabel{alg:acq1}{{4}{52}{Acquiring a batch of clip pairs with acquisition size 1.\relax }{algorithm.4}{}}
\newlabel{line:sample_exp2}{{1}{52}{Acquiring a batch of clip pairs with acquisition size 1.\relax }{algorithm.4}{}}
\newlabel{line:acq2}{{3}{52}{Acquiring a batch of clip pairs with acquisition size 1.\relax }{algorithm.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Hypothesis 5: Pool dataset has approximately uniform informativeness}{52}{section.6.4}\protected@file@percent }
\citation{Mnih2015}
\citation{kenton2019generalizing}
\newlabel{fig:sub1}{{6.6a}{54}{Standard CartPole\relax }{figure.caption.16}{}}
\newlabel{sub@fig:sub1}{{a}{54}{Standard CartPole\relax }{figure.caption.16}{}}
\newlabel{fig:sub2}{{6.6b}{54}{Repeated CartPole\relax }{figure.caption.16}{}}
\newlabel{sub@fig:sub2}{{b}{54}{Repeated CartPole\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Number of labels required to solve two variants of CartPole, using BALD and random acquisition functions for sampling clip pairs. Results are averaged over 20 repeats; error bars show standard error.\relax }}{54}{figure.caption.16}\protected@file@percent }
\newlabel{fig:cartpole_rep}{{6.6}{54}{Number of labels required to solve two variants of CartPole, using BALD and random acquisition functions for sampling clip pairs. Results are averaged over 20 repeats; error bars show standard error.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Gridworld experiments}{54}{section.6.5}\protected@file@percent }
\newlabel{fig:grida}{{6.7a}{55}{Gridworld with one goal cell.\relax }{figure.caption.17}{}}
\newlabel{sub@fig:grida}{{a}{55}{Gridworld with one goal cell.\relax }{figure.caption.17}{}}
\newlabel{fig:gridb}{{6.7b}{55}{Gridworld with one goal and one lava cell.\relax }{figure.caption.17}{}}
\newlabel{sub@fig:gridb}{{b}{55}{Gridworld with one goal and one lava cell.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Two examples of possible gridworld environments. The blue, green and red cells are the agent, goal and lava, respectively.\relax }}{55}{figure.caption.17}\protected@file@percent }
\newlabel{fig:grid}{{6.7}{55}{Two examples of possible gridworld environments. The blue, green and red cells are the agent, goal and lava, respectively.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Number of labels required to solve the gridworld shown in Figure \ref  {fig:grida} via reward modelling using BALD and random acquisition. Results shown are only one run.\relax }}{56}{figure.caption.18}\protected@file@percent }
\newlabel{fig:grid_simple}{{6.8}{56}{Number of labels required to solve the gridworld shown in Figure \ref {fig:grida} via reward modelling using BALD and random acquisition. Results shown are only one run.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Hypothesis 6: Active reward modelling exploration issues}{56}{section.6.6}\protected@file@percent }
\bibstyle{apalike}
\bibdata{library,additional}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusions}{57}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Summary}{57}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Future Work}{57}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Relation to Material Studied on the MSc Course}{57}{section.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Critical Evaluation}{57}{section.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Personal Development}{57}{subsection.7.4.1}\protected@file@percent }
\bibcite{Amodei2016}{Amodei et\nobreakspace  {}al., 2016}
\bibcite{bellemare2013arcade}{Bellemare et\nobreakspace  {}al., 2013}
\bibcite{Blundell2015}{Blundell et\nobreakspace  {}al., 2015}
\bibcite{gym2019cartpole}{Brockman et\nobreakspace  {}al., 2016a}
\bibcite{brockman2016gym}{Brockman et\nobreakspace  {}al., 2016b}
\bibcite{Byk2017}{B\IeC {\i }y\IeC {\i }k and Sadigh, 2017}
\bibcite{Christiano2017}{Christiano et\nobreakspace  {}al., 2017}
\bibcite{cohen2017emnist}{Cohen et\nobreakspace  {}al., 2017}
\bibcite{cohn1996active}{Cohn et\nobreakspace  {}al., 1996}
\@writefile{toc}{\contentsline {chapter}{References}{58}{chapter*.19}\protected@file@percent }
\bibcite{elo1978rating}{Elo, 1978}
\bibcite{freeman1965elementary}{Freeman, 1965}
\bibcite{fujimoto2018addressing}{Fujimoto et\nobreakspace  {}al., 2018}
\bibcite{Gal2017a}{Gal, 2017}
\bibcite{Gal2015}{Gal and Ghahramani, 2015}
\bibcite{Gal2017b}{Gal et\nobreakspace  {}al., 2017}
\bibcite{Goodfellow-et-al-2016}{Goodfellow et\nobreakspace  {}al., 2016}
\bibcite{haarnoja2018soft}{Haarnoja et\nobreakspace  {}al., 2018}
\bibcite{henderson2018deep}{Henderson et\nobreakspace  {}al., 2018}
\bibcite{Hester2017}{Hester et\nobreakspace  {}al., 2017}
\bibcite{Ho2016}{Ho and Ermon, 2016}
\bibcite{Houlsby2011}{Houlsby et\nobreakspace  {}al., 2011}
\bibcite{Ibarz2018}{Ibarz et\nobreakspace  {}al., 2018}
\bibcite{rlblogpost}{Irpan, 2018}
\bibcite{jones2001}{Jones et\nobreakspace  {}al., 2001}
\bibcite{kampffmeyer2016semantic}{Kampffmeyer et\nobreakspace  {}al., 2016}
\bibcite{kendall2015bayesian}{Kendall et\nobreakspace  {}al., 2015}
\bibcite{kenton2019generalizing}{Kenton et\nobreakspace  {}al., 2019}
\bibcite{Kirsch2019a}{Kirsch et\nobreakspace  {}al., 2019}
\bibcite{Knox2009}{Knox and Stone, 2009}
\bibcite{Leike2018}{Leike et\nobreakspace  {}al., 2018}
\bibcite{lillicrap2015continuous}{Lillicrap et\nobreakspace  {}al., 2015}
\bibcite{Mnih2016}{Mnih et\nobreakspace  {}al., 2016}
\bibcite{Mnih2015}{Mnih et\nobreakspace  {}al., 2015}
\bibcite{Ng2000}{Ng and Russell, 2000}
\bibcite{Palan2019}{Palan et\nobreakspace  {}al., 2019}
\bibcite{paszke2017automatic}{Paszke et\nobreakspace  {}al., 2017}
\bibcite{Schulman2015}{Schulman et\nobreakspace  {}al., 2015}
\bibcite{shannon1948mathematical}{Shannon, 1948}
\bibcite{silver2016mastering}{Silver et\nobreakspace  {}al., 2016}
\bibcite{Sutton2018}{Sutton and Barto, 2018}
\bibcite{tieleman2012lecture}{Tieleman and Hinton, 2012}
\bibcite{todorov2012mujoco}{Todorov et\nobreakspace  {}al., 2012}
\bibcite{van1995python}{Van\nobreakspace  {}Rossum and Drake\nobreakspace  {}Jr, 1995}
\bibcite{alphastarblog}{Vinyals et\nobreakspace  {}al., 2019}
\bibcite{Warnell2017}{Warnell et\nobreakspace  {}al., 2017}
\bibcite{Wilson2012}{Wilson et\nobreakspace  {}al., 2012}
\bibcite{Ziebart2008}{Ziebart et\nobreakspace  {}al., 2008}
\citation{Christiano2017}
\citation{Sutton2018}
\@writefile{toc}{\contentsline {chapter}{Appendices}{}{section*.20}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix A}{65}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:a}{{A}{65}{Appendix A}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}CartPole Experimental Details}{65}{section.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Ground truth reward function}{65}{subsection.A.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.2}Experiment 1 details}{65}{subsection.A.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.3}Other modifications made between testing hypothesis 4 and hypothesis 1 and 2}{66}{subsection.A.1.3}\protected@file@percent }
\newlabel{sec:other_changes}{{A.1.3}{66}{Other modifications made between testing hypothesis 4 and hypothesis 1 and 2}{subsection.A.1.3}{}}
