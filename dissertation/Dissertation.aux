\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Relation to Material Studied on the MSc Course}{3}{section.1.1}}
\citation{Sutton2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reinforcement Learning}{4}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Elements of Reinforcement Learning}{4}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Finite Markov Decision Processes}{4}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Agent-Environment Interface}{5}{subsection.2.2.1}}
\citation{Sutton2018}
\citation{Amodei2016}
\citation{Sutton2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Goals and Rewards}{6}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Returns and Episodes}{6}{subsection.2.2.3}}
\citation{Sutton2018}
\newlabel{G_t}{{2.1}{7}{(Future discounted) return}{equation.2.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Policies and Value Functions}{7}{subsection.2.2.4}}
\newlabel{policy_value_functions}{{2.2.4}{7}{Policies and Value Functions}{subsection.2.2.4}{}}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{Sutton2018}
\newlabel{v_pi}{{2.2}{8}{State-value function}{equation.2.2.2}{}}
\newlabel{v_pi}{{2.3}{8}{Action-value function}{equation.2.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Optimal Policies and Optimal Value Functions}{8}{subsection.2.2.5}}
\newlabel{optimal_policy_value_functions}{{2.2.5}{8}{Optimal Policies and Optimal Value Functions}{subsection.2.2.5}{}}
\newlabel{Q_claim}{{1}{8}{}{claim.1}{}}
\citation{Sutton2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Bellman Equations}{9}{subsection.2.2.6}}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{Sutton2018}
\citation{Achiam2019}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning Solution Methods}{10}{section.2.3}}
\newlabel{RL_solution_methods}{{2.3}{10}{Reinforcement Learning Solution Methods}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Taxonomy of RL Solution Methods}{11}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Deep Q-Learning}{11}{subsection.2.3.2}}
\newlabel{DQN}{{2.3.2}{11}{Deep Q-Learning}{subsection.2.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Reinforcement Learning from Unknown Reward Functions}{11}{section.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Reward Learning}{12}{subsection.2.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{In Standard RL}{12}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{In the Deep Case}{12}{section*.3}}
\citation{Gal2017a}
\citation{Gal2017a}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Uncertainty in Deep Learning}{13}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Active Learning}{14}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}Applying Active Learning to RL without a reward function}{14}{subsection.4.0.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Method}{15}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Experiments}{16}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Results}{17}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{MSc}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusions}{18}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Summary}{18}{section.8.1}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Evaluation}{18}{section.8.2}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Future Work}{18}{section.8.3}}
\bibcite{Achiam2019}{1}
\bibcite{Amodei2016}{2}
\bibcite{Gal2017a}{3}
\bibcite{Sutton2018}{4}
\@writefile{toc}{\contentsline {chapter}{References}{19}{chapter*.4}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{}{section*.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Some Appendix Material}{21}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
