Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Achiam2019,
author = {Achiam, Joshua},
file = {:C$\backslash$:/Users/Sam/Desktop/MSc/Project/Papers/openai-education-spinningup-latest.pdf:pdf},
title = {{Spinning Up in Deep RL}},
url = {http://spinningup.openai.com/en/latest/index.html},
urldate = {2019-08-01},
year = {2019}
}
@article{Gal2017a,
abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalchbrenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning. In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data efficient deep reinforcement learning. We further demonstrate the method's practicality through a survey of recent applications making use of the suggested tools in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore its theoretical implications, and the insights stemming from the link between Bayesian modelling and deep learning. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gal, Yarin},
doi = {10.1371/journal.pcbi.1005062},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gal - 2017 - Uncertainty in Deep Learning.pdf:pdf},
isbn = {9781509041176},
issn = {18736246},
journal = {Phd Thesis},
keywords = {2,Acoustic Event Detection,Acoustic Scene Classification,Acoustic event detection,Audio event detection,Autoencoder,Baum–Welch algorithm,Blur detection,Cepstrum,Convolutional Neural Networks,Convolutional neural network,Convolutional neural networks,Convolutional neural networks (CNNs),Deep Neural Networks,Deep belief network,Deep learning,Deep neural networks,Emotion recognition,Ex- treme learning machine,Glottal source estimation,Homomorphic processing,Incomplete data problem,Index Terms,Index Terms— Soundscape,Inverse filtering,Iteration algorithm,Local motion blur,Maximum a posteriori decoding,Maximum likelihood,Multipitch analysis,Pitch,Pitch visualization,Point spread function,Polyphonic music signals,Restricted Boltzmann machine,Speech,Speech analysis,Video Classification,[Electronic Manuscript],a block diagram of,applications,audio databases,audio noise,audio signal processing,audio-visual systems,auditory-visual information fusion,auto-encoder,autocorrelation method,bag-of-features,big data,bird,canonical correlation analysis,compuatational ecology,computer vision,computer vision aided,conceptually,convolutional neural networks,correlation theory,covariance method,cross-modal analysis,data collection,deep learning,deep neural networks,densenet,detection,dynamic audio-visual events,dynamic pixel filtering,echo planar imaging,emotion recognition,epi,field mapping,figure 1,filtering theory,first,formants,high resolution images,image resolution,in the feature extraction,instrument recognition,linear prediction,linear programming,manifold learning,microphone,model combination,module,multi-layer neural network,music information retrieval,neural networks,of three components,our system consists,phase retrieval,phase unwrapping,pitch-synchronous analysis,recurrent neural networks (RNNs),regularization,research,risks management,salience map,salient feature learning,signal reconstruction,sound event detection,sound ontology,source separation,speaker segmentation,spectrograms,spectrum,speech emotion recognition,spotting task,sustainable reconstruction,synthesis,the dnn kws system,traffic,u-net,unsupervised learning,used in this,video signal processing,visual distractions,visual event localization,work is shown in},
number = {1},
pages = {1--11},
pmid = {27178640},
title = {{Uncertainty in Deep Learning}},
url = {http://arxiv.org/abs/1608.04363{\%}0Ahttp://dx.doi.org/10.1109/LSP.2017.2657381{\%}0Ahttp://www.orangetide.com/smallc/articles/chaps/chap1/chap1.htm{\%}0Ahttp://131.111.33.187/groups/cnbh/research/publications/pdfs/SVOS Annex B 1988.pdf{\%}0Ahttp://www.mdpi.com/2079-},
volume = {1},
year = {2017}
}
@article{Akrour2012,
abstract = {This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert's ranking feedback enables the agent to refine the approximate policy return, and the process is iterated. In this paper, preference-based reinforcement learning is combined with active ranking in order to decrease the number of ranking queries to the expert needed to yield a satisfactory policy. Experiments on the mountain car and the cancer treatment testbeds witness that a couple of dozen rankings enable to learn a competent policy.},
archivePrefix = {arXiv},
arxivId = {arXiv:1208.0984v1},
author = {Akrour, Riad and Schoenauer, Marc and Sebag, Mich{\`{e}}le},
doi = {10.1007/978-3-642-33486-3_8},
eprint = {arXiv:1208.0984v1},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Akrour, Schoenauer, Sebag - 2012 - APRIL Active preference learning-based reinforcement learning.pdf:pdf},
isbn = {9783642334856},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {interactive optimization,preference learning,reinforcement learning,robotics},
number = {PART 2},
pages = {116--131},
title = {{APRIL: Active preference learning-based reinforcement learning}},
volume = {7524 LNAI},
year = {2012}
}
@article{Gal2015,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
eprint = {1506.02142},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gal, Ghahramani - 2015 - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf:pdf},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
url = {http://arxiv.org/abs/1506.02142},
volume = {48},
year = {2015}
}
@article{Leike2018,
abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
archivePrefix = {arXiv},
arxivId = {1811.07871},
author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
eprint = {1811.07871},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leike et al. - 2018 - Scalable agent alignment via reward modeling a research direction.pdf:pdf},
title = {{Scalable agent alignment via reward modeling: a research direction}},
url = {http://arxiv.org/abs/1811.07871},
year = {2018}
}
@article{Gal2017,
abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
archivePrefix = {arXiv},
arxivId = {1703.02910},
author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
eprint = {1703.02910},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gal, Islam, Ghahramani - 2017 - Deep Bayesian Active Learning with Image Data.pdf:pdf},
title = {{Deep Bayesian Active Learning with Image Data}},
url = {http://arxiv.org/abs/1703.02910},
year = {2017}
}
@article{Amodei2016,
abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
archivePrefix = {arXiv},
arxivId = {1606.06565},
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'{e}}, Dan},
eprint = {1606.06565},
file = {:C$\backslash$:/Users/Sam/Desktop/MSc/Project/Papers/Concrete Problems.pdf:pdf},
pages = {1--29},
title = {{Concrete Problems in AI Safety}},
url = {http://arxiv.org/abs/1606.06565},
year = {2016}
}
@article{Palan,
author = {Palan, Malayandi and Landolfi, Nicholas C and Shevchuk, Gleb and Sadigh, Dorsa},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Palan et al. - Unknown - Learning Reward Functions by Integrating Human Demonstrations and Preferences.pdf:pdf},
title = {{Learning Reward Functions by Integrating Human Demonstrations and Preferences}},
url = {http://ai.stanford.edu/blog/dempref/}
}
@article{Christiano2017,
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
archivePrefix = {arXiv},
arxivId = {1706.03741},
author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
eprint = {1706.03741},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:pdf},
title = {{Deep reinforcement learning from human preferences}},
url = {http://arxiv.org/abs/1706.03741},
year = {2017}
}
@book{Sutton2018,
author = {Sutton, R. S. and Barto, A. G.},
booktitle = {MIT Press},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton, Barto - 2018 - Reinforcement Learning An Introduction (2nd Edition, in preparation).pdf:pdf},
isbn = {9780262039246},
title = {{Reinforcement Learning: An Introduction (2nd Edition, in preparation)}},
url = {http://ir.obihiro.ac.jp/dspace/handle/10322/3933},
year = {2018}
}
@article{Ibarz2018,
abstract = {To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.},
archivePrefix = {arXiv},
arxivId = {1811.06521},
author = {Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
eprint = {1811.06521},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ibarz et al. - 2018 - Reward learning from human preferences and demonstrations in Atari.pdf:pdf},
number = {2017},
pages = {1--20},
title = {{Reward learning from human preferences and demonstrations in Atari}},
url = {http://arxiv.org/abs/1811.06521},
year = {2018}
}
@article{Osband2016,
abstract = {Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.04621},
author = {Osband, Ian},
doi = {10.1145/2661829.2661935},
eprint = {arXiv:1602.04621},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Osband - 2016 - Risk versus Uncertainty in Deep Learning Bayes , Bootstrap and the Dangers of Dropout.pdf:pdf},
isbn = {9781479969227},
issn = {10495258},
journal = {NIPS 2016 Bayesian Deep Learning Workshop},
pages = {26--28},
pmid = {15003161},
title = {{Risk versus Uncertainty in Deep Learning : Bayes , Bootstrap and the Dangers of Dropout}},
url = {http://bayesiandeeplearning.org/2016/papers/BDL{\_}4.pdf},
year = {2016}
}
@article{Houlsby2011,
abstract = {Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.},
archivePrefix = {arXiv},
arxivId = {1112.5745},
author = {Houlsby, Neil and Husz{\'{a}}r, Ferenc and Ghahramani, Zoubin and Lengyel, M{\'{a}}t{\'{e}}},
eprint = {1112.5745},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Houlsby et al. - 2011 - Bayesian Active Learning for Classification and Preference Learning.pdf:pdf},
pages = {1--17},
title = {{Bayesian Active Learning for Classification and Preference Learning}},
url = {http://arxiv.org/abs/1112.5745},
year = {2011}
}
@article{Byk2018,
abstract = {Data generation and labeling are usually an expensive part of learning for robotics. While active learning methods are commonly used to tackle the former problem, preference-based learning is a concept that attempts to solve the latter by querying users with preference questions. In this paper, we will develop a new algorithm, batch active preference-based learning, that enables efficient learning of reward functions using as few data samples as possible while still having short query generation times. We introduce several approximations to the batch active learning problem, and provide theoretical guarantees for the convergence of our algorithms. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We then showcase our algorithm in a study to learn human users' preferences.},
archivePrefix = {arXiv},
arxivId = {1810.04303},
author = {Bıyık, Erdem and Sadigh, Dorsa},
eprint = {1810.04303},
file = {:C$\backslash$:/Users/Sam/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bıyık, Sadigh - 2018 - Batch Active Preference-Based Learning of Reward Functions.pdf:pdf},
title = {{Batch Active Preference-Based Learning of Reward Functions}},
url = {http://arxiv.org/abs/1810.04303},
year = {2018}
}
