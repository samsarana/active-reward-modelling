Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Kirsch2019,
abstract = {We develop BatchBALD, a tractable approximation to the mutual information between a batch of points and model parameters, which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. BatchBALD is a greedy linear-time {\$}1 - \backslashfrac{\{}1{\}}{\{}e{\}}{\$}-approximate algorithm amenable to dynamic programming and efficient caching. We compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. We finish by showing that, using BatchBALD to consider dependencies within an acquisition batch, we achieve new state of the art performance on standard benchmarks, providing substantial data efficiency improvements in batch acquisition.},
archivePrefix = {arXiv},
arxivId = {1906.08158},
author = {Kirsch, Andreas and van Amersfoort, Joost and Gal, Yarin},
eprint = {1906.08158},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/1906.08158.pdf:pdf},
title = {{BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning}},
url = {http://arxiv.org/abs/1906.08158},
year = {2019}
}
@article{Ziebart2008,
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories. Copyright ? 2008.},
author = {Ziebart, B.D. and Maas, Andrew and Bagnell, J.A. and Dey, A.K.},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Maximum{\_}Entropy{\_}Inverse{\_}Reinforcement{\_}Learning.pdf:pdf},
journal = {Proc. AAAI},
number = {January},
pages = {1433--1438},
title = {{Maximum entropy inverse reinforcement learning}},
url = {http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf},
year = {2008}
}
@article{Amodei2016,
abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
archivePrefix = {arXiv},
arxivId = {1606.06565},
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'{e}}, Dan},
eprint = {1606.06565},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:pdf},
pages = {1--29},
title = {{Concrete Problems in AI Safety}},
url = {http://arxiv.org/abs/1606.06565},
year = {2016}
}
@article{Christiano2017,
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
archivePrefix = {arXiv},
arxivId = {1706.03741},
author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
eprint = {1706.03741},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:pdf},
title = {{Deep reinforcement learning from human preferences}},
url = {http://arxiv.org/abs/1706.03741},
year = {2017}
}
@article{Gal2017a,
abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalchbrenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning. In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data efficient deep reinforcement learning. We further demonstrate the method's practicality through a survey of recent applications making use of the suggested tools in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore its theoretical implications, and the insights stemming from the link between Bayesian modelling and deep learning. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gal, Yarin},
doi = {10.1371/journal.pcbi.1005062},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Gal - 2017 - Uncertainty in Deep Learning.pdf:pdf},
isbn = {9781509041176},
issn = {18736246},
journal = {Phd Thesis},
keywords = {2,Acoustic Event Detection,Acoustic Scene Classification,Acoustic event detection,Audio event detection,Autoencoder,Baum–Welch algorithm,Blur detection,Cepstrum,Convolutional Neural Networks,Convolutional neural network,Convolutional neural networks,Convolutional neural networks (CNNs),Deep Neural Networks,Deep belief network,Deep learning,Deep neural networks,Emotion recognition,Ex- treme learning machine,Glottal source estimation,Homomorphic processing,Incomplete data problem,Index Terms,Index Terms— Soundscape,Inverse filtering,Iteration algorithm,Local motion blur,Maximum a posteriori decoding,Maximum likelihood,Multipitch analysis,Pitch,Pitch visualization,Point spread function,Polyphonic music signals,Restricted Boltzmann machine,Speech,Speech analysis,Video Classification,[Electronic Manuscript],a block diagram of,applications,audio databases,audio noise,audio signal processing,audio-visual systems,auditory-visual information fusion,auto-encoder,autocorrelation method,bag-of-features,big data,bird,canonical correlation analysis,compuatational ecology,computer vision,computer vision aided,conceptually,convolutional neural networks,correlation theory,covariance method,cross-modal analysis,data collection,deep learning,deep neural networks,densenet,detection,dynamic audio-visual events,dynamic pixel filtering,echo planar imaging,emotion recognition,epi,field mapping,figure 1,filtering theory,first,formants,high resolution images,image resolution,in the feature extraction,instrument recognition,linear prediction,linear programming,manifold learning,microphone,model combination,module,multi-layer neural network,music information retrieval,neural networks,of three components,our system consists,phase retrieval,phase unwrapping,pitch-synchronous analysis,recurrent neural networks (RNNs),regularization,research,risks management,salience map,salient feature learning,signal reconstruction,sound event detection,sound ontology,source separation,speaker segmentation,spectrograms,spectrum,speech emotion recognition,spotting task,sustainable reconstruction,synthesis,the dnn kws system,traffic,u-net,unsupervised learning,used in this,video signal processing,visual distractions,visual event localization,work is shown in},
number = {1},
pages = {1--11},
pmid = {27178640},
title = {{Uncertainty in Deep Learning}},
url = {http://arxiv.org/abs/1608.04363{\%}0Ahttp://dx.doi.org/10.1109/LSP.2017.2657381{\%}0Ahttp://www.orangetide.com/smallc/articles/chaps/chap1/chap1.htm{\%}0Ahttp://131.111.33.187/groups/cnbh/research/publications/pdfs/SVOS Annex B 1988.pdf{\%}0Ahttp://www.mdpi.com/2079-},
volume = {1},
year = {2017}
}
@article{Osband2016,
abstract = {Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.04621},
author = {Osband, Ian},
doi = {10.1145/2661829.2661935},
eprint = {arXiv:1602.04621},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Osband - 2016 - Risk versus Uncertainty in Deep Learning Bayes , Bootstrap and the Dangers of Dropout.pdf:pdf},
isbn = {9781479969227},
issn = {10495258},
journal = {NIPS 2016 Bayesian Deep Learn. Work.},
pages = {26--28},
pmid = {15003161},
title = {{Risk versus Uncertainty in Deep Learning : Bayes , Bootstrap and the Dangers of Dropout}},
url = {http://bayesiandeeplearning.org/2016/papers/BDL{\_}4.pdf},
year = {2016}
}
@article{Wilson2012,
abstract = {We consider the problem of learning control policies via trajectory preference queries to an expert. In particular, the agent presents an expert with short runs of a pair of policies originating from the same state and the expert indicates which trajectory is preferred. The agent's goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problem we propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries. Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efficient than random selection.},
author = {Wilson, A. and Fern, A. and Tadepalli, P.},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/4805-a-bayesian-approach-for-policy-learning-from-trajectory-preference-queries.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {1--9},
title = {{A Bayesian approach for policy learning from trajectory preference queries}},
volume = {2},
year = {2012}
}
@article{Hester2017,
abstract = {Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrator's actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD's performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.},
archivePrefix = {arXiv},
arxivId = {1704.03732},
author = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Dulac-Arnold, Gabriel and Osband, Ian and Agapiou, John and Leibo, Joel Z. and Gruslys, Audrunas},
eprint = {1704.03732},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/1704.03732.pdf:pdf},
title = {{Deep Q-learning from Demonstrations}},
url = {http://arxiv.org/abs/1704.03732},
year = {2017}
}
@article{Houlsby2011,
abstract = {Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.},
archivePrefix = {arXiv},
arxivId = {1112.5745},
author = {Houlsby, Neil and Husz{\'{a}}r, Ferenc and Ghahramani, Zoubin and Lengyel, M{\'{a}}t{\'{e}}},
eprint = {1112.5745},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Houlsby et al. - 2011 - Bayesian Active Learning for Classification and Preference Learning.pdf:pdf},
pages = {1--17},
title = {{Bayesian Active Learning for Classification and Preference Learning}},
url = {http://arxiv.org/abs/1112.5745},
year = {2011}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/nature14236.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Leike2018,
abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
archivePrefix = {arXiv},
arxivId = {1811.07871},
author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
eprint = {1811.07871},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Leike et al. - 2018 - Scalable agent alignment via reward modeling a research direction.pdf:pdf},
title = {{Scalable agent alignment via reward modeling: a research direction}},
url = {http://arxiv.org/abs/1811.07871},
year = {2018}
}
@article{Sadigh2018,
abstract = {Data generation and labeling are usually an expensive part of learning for robotics. While active learning methods are commonly used to tackle the former problem, preference-based learning is a concept that attempts to solve the latter by querying users with preference questions. In this paper, we will develop a new algorithm, batch active preference-based learning, that enables efficient learning of reward functions using as few data samples as possible while still having short query generation times. We introduce several approximations to the batch active learning problem, and provide theoretical guarantees for the convergence of our algorithms. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We then showcase our algorithm in a study to learn human users' preferences.},
archivePrefix = {arXiv},
arxivId = {arXiv:1810.04303v1},
author = {Sadigh, Dorsa and Dragan, Anca and Sastry, Shankar and Seshia, Sanjit},
doi = {10.15607/RSS.2017.XIII.053},
eprint = {arXiv:1810.04303v1},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/1810.04303.pdf:pdf},
isbn = {978-0-9923747-3-0},
journal = {Robot. Sci. Syst. XIII},
keywords = {active learning,batch active,pool based active,preference learning},
number = {CoRL},
title = {{Batch Active Preference-Based Learning of Reward Functions}},
url = {http://www.roboticsproceedings.org/rss13/p53.pdf},
year = {2018}
}
@article{Schulman2015,
abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
eprint = {1502.05477},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/1502.05477.pdf:pdf},
title = {{Trust Region Policy Optimization}},
url = {http://arxiv.org/abs/1502.05477},
year = {2015}
}
@article{Arumugam2019,
abstract = {To widen their accessibility and increase their utility, intelligent agents must be able to learn complex behaviors as specified by (non-expert) human users. Moreover, they will need to learn these behaviors within a reasonable amount of time while efficiently leveraging the sparse feedback a human trainer is capable of providing. Recent work has shown that human feedback can be characterized as a critique of an agent's current behavior rather than as an alternative reward signal to be maximized, culminating in the COnvergent Actor-Critic by Humans (COACH) algorithm for making direct policy updates based on human feedback. Our work builds on COACH, moving to a setting where the agent's policy is represented by a deep neural network. We employ a series of modifications on top of the original COACH algorithm that are critical for successfully learning behaviors from high-dimensional observations, while also satisfying the constraint of obtaining reduced sample complexity. We demonstrate the effectiveness of our Deep COACH algorithm in the rich 3D world of Minecraft with an agent that learns to complete tasks by mapping from raw pixels to actions using only real-time human feedback in 10-15 minutes of interaction.},
archivePrefix = {arXiv},
arxivId = {1902.04257},
author = {Arumugam, Dilip and Lee, Jun Ki and Saskin, Sophie and Littman, Michael L.},
eprint = {1902.04257},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/1902.04257.pdf:pdf},
title = {{Deep Reinforcement Learning from Policy-Dependent Human Feedback}},
url = {http://arxiv.org/abs/1902.04257},
year = {2019}
}
@article{Gal2015,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
eprint = {1506.02142},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Gal, Ghahramani - 2015 - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf:pdf},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
url = {http://arxiv.org/abs/1506.02142},
volume = {48},
year = {2015}
}
@article{Gal2017b,
abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
archivePrefix = {arXiv},
arxivId = {1703.02910},
author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
eprint = {1703.02910},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Gal, Islam, Ghahramani - 2017 - Deep Bayesian Active Learning with Image Data.pdf:pdf},
title = {{Deep Bayesian Active Learning with Image Data}},
url = {http://arxiv.org/abs/1703.02910},
year = {2017}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1505.05424},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:pdf},
title = {{Weight Uncertainty in Neural Networks}},
url = {http://arxiv.org/abs/1505.05424},
volume = {37},
year = {2015}
}
@article{Warnell2017,
abstract = {While recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks, existing algorithms generally require a lot of training data. One way to increase the speed at which agents are able to learn to perform tasks is by leveraging the input of human trainers. Although such input can take many forms, real-time, scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations. Previous approaches have shown the usefulness of human input provided in this fashion (e.g., the TAMER framework), but they have thus far not considered high-dimensional state spaces or employed the use of deep learning. In this paper, we do both: we propose Deep TAMER, an extension of the TAMER framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer. We demonstrate Deep TAMER's success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of Bowling - a task that has proven difficult for even state-of-the-art reinforcement learning methods.},
archivePrefix = {arXiv},
arxivId = {1709.10163},
author = {Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter},
eprint = {1709.10163},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/16200-76612-1-PB.pdf:pdf},
keywords = {Human AI Collaboration Track},
pages = {1545--1553},
title = {{Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces}},
url = {http://arxiv.org/abs/1709.10163},
year = {2017}
}
@article{Ng2000,
abstract = {This paper addresses the problem of inverse reinforcement learning (IRL) in Markov decision processes, that is, the problem of extracting a reward function given observed, optimal behaviour. IRL may be useful for apprenticeship learning to acquire skilled behaviour, and for ascertaining the reward function being optimized by a natural system. We rst characterize the set of all reward functions for which a given policy is optimal. We then derive three algorithms for IRL. The rst two deal with the case where the entire policy is known; we handle tabulated reward functions on a nite state space and linear functional approximation of the reward function over a potentially in- nite state space. The third algorithm deals with the more realistic case in which the policy is known only through a nite set of observed trajectories. In all cases, a key issue is degeneracythe existence of a large set of reward functions for which the observed policy is optimal. To remove...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ng, Andrew and Russell, Stuart},
doi = {10.2460/ajvr.67.2.323},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/icml00-irl.pdf:pdf},
isbn = {1-55860-707-2},
issn = {00029645},
journal = {Proc. Seventeenth Int. Conf. Mach. Learn.},
pages = {663--670},
pmid = {16454640},
title = {{Algorithms for inverse reinforcement learning}},
url = {http://www-cs.stanford.edu/people/ang/papers/icml00-irl.pdf},
volume = {0},
year = {2000}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/1602.01783.pdf:pdf},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@article{Byk2017,
abstract = {Data generation and labeling are usually an expensive part of learning for robotics. While active learning methods are commonly used to tackle the former problem, preference-based learning is a concept that attempts to solve the latter by querying users with preference questions. In this paper, we will develop a new algorithm, batch active preference-based learning, that enables efficient learning of reward functions using as few data samples as possible while still having short query generation times. We introduce several approximations to the batch active learning problem, and provide theoretical guarantees for the convergence of our algorithms. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We then showcase our algorithm in a study to learn human users' preferences.},
archivePrefix = {arXiv},
arxivId = {1810.04303},
author = {Bıyık, Erdem and Sadigh, Dorsa},
eprint = {1810.04303},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Bıyık, Sadigh - 2018 - Active Preference-Based Learning of Reward Functions.pdf:pdf},
title = {{Active Preference-Based Learning of Reward Functions}},
url = {http://arxiv.org/abs/1810.04303},
year = {2017}
}
@book{Sutton2018,
author = {Sutton, R. S. and Barto, A. G.},
booktitle = {MIT Press},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Sutton, Barto - 2018 - Reinforcement Learning An Introduction (2nd Edition, in preparation).pdf:pdf},
isbn = {9780262039246},
title = {{Reinforcement Learning: An Introduction (2nd Edition, in preparation)}},
url = {http://ir.obihiro.ac.jp/dspace/handle/10322/3933},
year = {2018}
}
@article{Akrour2012,
abstract = {This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert's ranking feedback enables the agent to refine the approximate policy return, and the process is iterated. In this paper, preference-based reinforcement learning is combined with active ranking in order to decrease the number of ranking queries to the expert needed to yield a satisfactory policy. Experiments on the mountain car and the cancer treatment testbeds witness that a couple of dozen rankings enable to learn a competent policy.},
archivePrefix = {arXiv},
arxivId = {arXiv:1208.0984v1},
author = {Akrour, Riad and Schoenauer, Marc and Sebag, Mich{\`{e}}le},
doi = {10.1007/978-3-642-33486-3_8},
eprint = {arXiv:1208.0984v1},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Akrour, Schoenauer, Sebag - 2012 - APRIL Active preference learning-based reinforcement learning.pdf:pdf},
isbn = {9783642334856},
issn = {03029743},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
keywords = {interactive optimization,preference learning,reinforcement learning,robotics},
number = {PART 2},
pages = {116--131},
title = {{APRIL: Active preference learning-based reinforcement learning}},
volume = {7524 LNAI},
year = {2012}
}
@article{Knox2009,
abstract = {As computational learning agents move into domains that incur real costs (e.g., autonomous driving or ﬁnancial investment), it will be necessary to learn good policies without numerous high-cost learning trials. One promising approach to reducing sample complexity of learning a task is knowledge transfer from humans to agents. Ideally, methods of transfer should be accessible to anyone with task knowledge, regardless of that person's expertise in programming and AI. This paper focuses on allowing a human trainer to interactively shape an agent's policy via reinforcement signals. Speciﬁcally, the paper introduces “Training an Agent Manually via Evaluative Reinforcement,” or tamer, a framework that enables such shaping. Diﬀering from previous approaches to interactive shaping, a tamer agent models the human's reinforcement and exploits its model by choosing actions expected to be most highly reinforced. Results from two domains demonstrate that lay users can train tamer agents without deﬁning an environmental reward function (as in an MDP) and indicate that human training within the tamer framework can reduce sample complexity over autonomous learning algorithms.},
author = {Knox, W Bradley and Stone, Peter},
doi = {10.1145/1597735.1597738},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Knox09.pdf:pdf},
isbn = {9781605586588},
journal = {Proc. fifth Int. Conf. Knowl. capture - K-CAP '09},
keywords = {cs,deploy these agents in,edu,human teachers,human-agent interaction,learning agents,making de-,pstone,real-world domains,sequen-,shaping,tial decision-making,utexas},
pages = {9},
title = {{Interactively shaping agents via human reinforcement: The Tamer Framework}},
url = {http://portal.acm.org/citation.cfm?doid=1597735.1597738},
year = {2009}
}
@article{Ho2016,
abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
archivePrefix = {arXiv},
arxivId = {1606.03476},
author = {Ho, Jonathan and Ermon, Stefano},
eprint = {1606.03476},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/1606.03476.pdf:pdf},
title = {{Generative Adversarial Imitation Learning}},
url = {http://arxiv.org/abs/1606.03476},
year = {2016}
}
@article{Palan2019,
abstract = {Our goal is to accurately and efficiently learn reward functions for autonomous robots. Current approaches to this problem include inverse reinforcement learning (IRL), which uses expert demonstrations, and preference-based learning, which iteratively queries the user for her preferences between trajectories. In robotics however, IRL often struggles because it is difficult to get high-quality demonstrations; conversely, preference-based learning is very inefficient since it attempts to learn a continuous, high-dimensional function from binary feedback. We propose a new framework for reward learning, DemPref, that uses both demonstrations and preference queries to learn a reward function. Specifically, we (1) use the demonstrations to learn a coarse prior over the space of reward functions, to reduce the effective size of the space from which queries are generated; and (2) use the demonstrations to ground the (active) query generation process, to improve the quality of the generated queries. Our method alleviates the efficiency issues faced by standard preference-based learning methods and does not exclusively depend on (possibly low-quality) demonstrations. In numerical experiments, we find that DemPref is significantly more efficient than a standard active preference-based learning method. In a user study, we compare our method to a standard IRL method; we find that users rated the robot trained with DemPref as being more successful at learning their desired behavior, and preferred to use the DemPref system (over IRL) to train the robot.},
archivePrefix = {arXiv},
arxivId = {1906.08928},
author = {Palan, Malayandi and Landolfi, Nicholas C. and Shevchuk, Gleb and Sadigh, Dorsa},
eprint = {1906.08928},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Palan et al. - 2019 - Learning Reward Functions by Integrating Human Demonstrations and Preferences.pdf:pdf},
title = {{[DemPref] Learning Reward Functions by Integrating Human Demonstrations and Preferences}},
url = {http://arxiv.org/abs/1906.08928},
year = {2019}
}
@article{Ibarz2018,
abstract = {To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.},
archivePrefix = {arXiv},
arxivId = {1811.06521},
author = {Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
eprint = {1811.06521},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Ibarz et al. - 2018 - Reward learning from human preferences and demonstrations in Atari.pdf:pdf},
number = {2017},
pages = {1--20},
title = {{Reward learning from human preferences and demonstrations in Atari}},
url = {http://arxiv.org/abs/1811.06521},
year = {2018}
}
@misc{Achiam2019,
author = {Achiam, Joshua},
file = {:C$\backslash$:/Users/samck/MSc/Project/Papers/Achiam - 2019 - Spinning Up in Deep RL.pdf:pdf},
title = {{Spinning Up in Deep RL}},
url = {http://spinningup.openai.com/en/latest/index.html},
urldate = {2019-08-01},
year = {2019}
}
