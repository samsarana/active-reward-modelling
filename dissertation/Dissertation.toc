\contentsline {chapter}{\numberline {1}Introduction}{4}{chapter.1}% 
\contentsline {section}{\numberline {1.1}Relation to Material Studied on the MSc Course}{4}{section.1.1}% 
\contentsline {chapter}{\numberline {2}Reinforcement Learning}{5}{chapter.2}% 
\contentsline {section}{\numberline {2.1}Elements of Reinforcement Learning}{5}{section.2.1}% 
\contentsline {section}{\numberline {2.2}Finite Markov Decision Processes}{5}{section.2.2}% 
\contentsline {subsection}{\numberline {2.2.1}The Agent-Environment Interface}{6}{subsection.2.2.1}% 
\contentsline {subsection}{\numberline {2.2.2}Goals and Rewards}{7}{subsection.2.2.2}% 
\contentsline {subsection}{\numberline {2.2.3}Returns and Episodes}{7}{subsection.2.2.3}% 
\contentsline {subsection}{\numberline {2.2.4}Policies and Value Functions}{8}{subsection.2.2.4}% 
\contentsline {subsection}{\numberline {2.2.5}Optimal Policies and Optimal Value Functions}{9}{subsection.2.2.5}% 
\contentsline {subsection}{\numberline {2.2.6}Bellman Equations}{10}{subsection.2.2.6}% 
\contentsline {section}{\numberline {2.3}Reinforcement Learning Solution Methods}{11}{section.2.3}% 
\contentsline {subsection}{\numberline {2.3.1}Taxonomy of RL Solution Methods}{12}{subsection.2.3.1}% 
\contentsline {subsection}{\numberline {2.3.2}Deep Q-Learning}{12}{subsection.2.3.2}% 
\contentsline {section}{\numberline {2.4}Reinforcement Learning from Unknown Reward Functions}{12}{section.2.4}% 
\contentsline {subsection}{\numberline {2.4.1}Reward Learning}{13}{subsection.2.4.1}% 
\contentsline {subsubsection}{In Classical RL}{13}{section*.2}% 
\contentsline {subsubsection}{In Deep RL}{13}{section*.3}% 
\contentsline {chapter}{\numberline {3}Uncertainty in Deep Learning}{14}{chapter.3}% 
\contentsline {section}{\numberline {3.1}Bayesian Neural Networks}{15}{section.3.1}% 
\contentsline {section}{\numberline {3.2}Model Uncertainty in BNNs}{15}{section.3.2}% 
\contentsline {chapter}{\numberline {4}Active Learning}{16}{chapter.4}% 
\contentsline {section}{\numberline {4.1}Acquisition Functions}{16}{section.4.1}% 
\contentsline {subsection}{\numberline {4.1.1}Max Entropy}{16}{subsection.4.1.1}% 
\contentsline {subsection}{\numberline {4.1.2}Variation Ratios}{16}{subsection.4.1.2}% 
\contentsline {subsection}{\numberline {4.1.3}Mean STD}{16}{subsection.4.1.3}% 
\contentsline {subsection}{\numberline {4.1.4}BALD}{16}{subsection.4.1.4}% 
\contentsline {section}{\numberline {4.2}Applying Active Learning to RL without a reward function}{16}{section.4.2}% 
\contentsline {subsection}{\numberline {4.2.1}APRIL}{16}{subsection.4.2.1}% 
\contentsline {subsection}{\numberline {4.2.2}Batch active preference-based learning}{16}{subsection.4.2.2}% 
\contentsline {subsection}{\numberline {4.2.3}DemPref}{16}{subsection.4.2.3}% 
\contentsline {subsection}{\numberline {4.2.4}Deep RL from Human Preferences}{16}{subsection.4.2.4}% 
\contentsline {chapter}{\numberline {5}Method}{17}{chapter.5}% 
\contentsline {section}{\numberline {5.1}Training Protocol}{17}{section.5.1}% 
\contentsline {section}{\numberline {5.2}Acquisition Functions}{17}{section.5.2}% 
\contentsline {section}{\numberline {5.3}Uncertainty Estimates}{17}{section.5.3}% 
\contentsline {section}{\numberline {5.4}Implementation Details}{17}{section.5.4}% 
\contentsline {chapter}{\numberline {6}Experiments}{18}{chapter.6}% 
\contentsline {section}{\numberline {6.1}CartPole-v0}{18}{section.6.1}% 
\contentsline {chapter}{\numberline {7}Results}{19}{chapter.7}% 
\contentsline {section}{\numberline {7.1}CartPole-v0}{19}{section.7.1}% 
\contentsline {chapter}{\numberline {8}Conclusions}{20}{chapter.8}% 
\contentsline {section}{\numberline {8.1}Summary}{20}{section.8.1}% 
\contentsline {section}{\numberline {8.2}Evaluation}{20}{section.8.2}% 
\contentsline {section}{\numberline {8.3}Future Work}{20}{section.8.3}% 
\contentsline {chapter}{References}{21}{chapter*.4}% 
\contentsline {chapter}{Appendices}{}{section*.5}
\contentsline {chapter}{\numberline {A}Some Appendix Material}{23}{appendix.A}% 
