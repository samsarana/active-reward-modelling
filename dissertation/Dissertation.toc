\contentsline {part}{I\hspace {1em}Background}{4}{part.1}% 
\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}% 
\contentsline {chapter}{\numberline {2}Reinforcement Learning}{8}{chapter.2}% 
\contentsline {section}{\numberline {2.1}Elements of Reinforcement Learning}{8}{section.2.1}% 
\contentsline {section}{\numberline {2.2}Finite Markov Decision Processes}{8}{section.2.2}% 
\contentsline {subsection}{\numberline {2.2.1}The Agent-Environment Interface}{9}{subsection.2.2.1}% 
\contentsline {subsection}{\numberline {2.2.2}Goals and Rewards}{10}{subsection.2.2.2}% 
\contentsline {subsection}{\numberline {2.2.3}Returns and Episodes}{10}{subsection.2.2.3}% 
\contentsline {subsection}{\numberline {2.2.4}Policies and Value Functions}{11}{subsection.2.2.4}% 
\contentsline {subsection}{\numberline {2.2.5}Optimal Policies and Optimal Value Functions}{12}{subsection.2.2.5}% 
\contentsline {subsection}{\numberline {2.2.6}Bellman Equations}{13}{subsection.2.2.6}% 
\contentsline {section}{\numberline {2.3}Reinforcement Learning Solution Methods}{14}{section.2.3}% 
\contentsline {subsection}{\numberline {2.3.1}Deep Neural Networks}{15}{subsection.2.3.1}% 
\contentsline {subsubsection}{Cost function}{16}{section*.2}% 
\contentsline {subsubsection}{Optimization procedure}{17}{section*.3}% 
\contentsline {subsection}{\numberline {2.3.2}Deep Q-network}{19}{subsection.2.3.2}% 
\contentsline {section}{\numberline {2.4}Reinforcement Learning from Unknown Reward Functions}{21}{section.2.4}% 
\contentsline {subsection}{\numberline {2.4.1}Reward Learning from Trajectory Preferences in Deep RL}{24}{subsection.2.4.1}% 
\contentsline {subsubsection}{Setting}{24}{section*.4}% 
\contentsline {subsubsection}{Method}{25}{section*.5}% 
\contentsline {subsubsection}{Process 1: Training the policy}{26}{section*.6}% 
\contentsline {subsubsection}{Process 2: Selecting and annotating clip pairs}{26}{section*.7}% 
\contentsline {subsubsection}{Process 3: Training the reward model}{27}{section*.8}% 
\contentsline {subsection}{\numberline {2.4.2}Reward Learning from Trajectory Preferences with Handcrafted Feature Transformations}{28}{subsection.2.4.2}% 
\contentsline {subsubsection}{Setting}{28}{section*.9}% 
\contentsline {subsubsection}{Method}{28}{section*.10}% 
\contentsline {chapter}{\numberline {3}Uncertainty in Deep Learning}{30}{chapter.3}% 
\contentsline {section}{\numberline {3.1}Bayesian Neural Networks}{31}{section.3.1}% 
\contentsline {section}{\numberline {3.2}Active Learning}{32}{section.3.2}% 
\contentsline {subsection}{\numberline {3.2.1}Max Entropy}{33}{subsection.3.2.1}% 
\contentsline {subsection}{\numberline {3.2.2}BALD}{33}{subsection.3.2.2}% 
\contentsline {subsection}{\numberline {3.2.3}Variation Ratios}{36}{subsection.3.2.3}% 
\contentsline {subsection}{\numberline {3.2.4}Mean STD}{36}{subsection.3.2.4}% 
\contentsline {chapter}{\numberline {4}Applying Active Learning to RL without a reward function}{38}{chapter.4}% 
\contentsline {subsection}{\numberline {4.0.1}APRIL}{38}{subsection.4.0.1}% 
\contentsline {subsection}{\numberline {4.0.2}Active Preference-Based Learning of Reward Functions with handcrafted feature transformations}{38}{subsection.4.0.2}% 
\contentsline {subsection}{\numberline {4.0.3}Deep RL from Human Preferences}{38}{subsection.4.0.3}% 
\contentsline {part}{II\hspace {1em}Innovation}{39}{part.2}% 
\contentsline {chapter}{\numberline {5}Method}{40}{chapter.5}% 
\contentsline {section}{\numberline {5.1}Possible failure modes of active reward modelling}{40}{section.5.1}% 
\contentsline {subsection}{\numberline {5.1.1}Failure modes of active learning in general}{41}{subsection.5.1.1}% 
\contentsline {subsection}{\numberline {5.1.2}Failure modes of active learning in the reward modelling setting}{42}{subsection.5.1.2}% 
\contentsline {section}{\numberline {5.2}Active Reward Modelling}{44}{section.5.2}% 
\contentsline {section}{\numberline {5.3}Acquisition Functions and Uncertainty Estimates}{45}{section.5.3}% 
\contentsline {section}{\numberline {5.4}Implementation Details}{45}{section.5.4}% 
\contentsline {chapter}{\numberline {6}Experiments and Results}{46}{chapter.6}% 
\contentsline {section}{\numberline {6.1}Hypothesis 1: Reward model retraining}{48}{section.6.1}% 
\contentsline {section}{\numberline {6.2}Hypothesis 2: Acquisition size}{49}{section.6.2}% 
\contentsline {section}{\numberline {6.3}Hypotheses 3 and 4: Uncertainty estimate method and acquisition functions}{49}{section.6.3}% 
\contentsline {section}{\numberline {6.4}Hypotheses 5 and 6: Quality of uncertainty estimates and ease of learning reward model}{52}{section.6.4}% 
\contentsline {section}{\numberline {6.5}Gridworld experiments}{55}{section.6.5}% 
\contentsline {section}{\numberline {6.6}Uncertainty estimate quality in the gridworld environment}{56}{section.6.6}% 
\contentsline {section}{\numberline {6.7}Hypothesis 7: Active reward modelling exploration issues}{58}{section.6.7}% 
\contentsline {chapter}{\numberline {7}Conclusions}{60}{chapter.7}% 
\contentsline {section}{\numberline {7.1}Summary}{60}{section.7.1}% 
\contentsline {section}{\numberline {7.2}Future Work}{60}{section.7.2}% 
\contentsline {section}{\numberline {7.3}Relation to Material Studied on the MSc Course}{60}{section.7.3}% 
\contentsline {section}{\numberline {7.4}Critical Evaluation}{60}{section.7.4}% 
\contentsline {subsection}{\numberline {7.4.1}Personal Development}{60}{subsection.7.4.1}% 
\contentsline {chapter}{References}{61}{chapter*.22}% 
\contentsline {chapter}{Appendices}{}{section*.23}
\contentsline {chapter}{\numberline {A}Appendix A}{68}{appendix.A}% 
\contentsline {section}{\numberline {A.1}CartPole Experimental Details}{68}{section.A.1}% 
\contentsline {subsection}{\numberline {A.1.1}Ground truth reward function}{68}{subsection.A.1.1}% 
\contentsline {subsection}{\numberline {A.1.2}Experiment 1 details}{68}{subsection.A.1.2}% 
\contentsline {subsection}{\numberline {A.1.3}Other modifications made between testing hypothesis 4 and hypothesis 1 and 2}{69}{subsection.A.1.3}% 
